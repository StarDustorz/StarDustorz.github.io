---
title: 分布式理论
date: 2021-06-12 19:29:17
tags:
  - System-Design
categories:
  - System-Design
published: false
toc: "true"
comments: "true"
description: 
---

> 分布式实际上就是单一的本地一体解决方案，在硬件或者资源上不够业务需求，而采取的一种分散式多节点，可以扩容资源的一种解决思路。
## 1 一、从本地事务到分布式理论
> 事务提供一种机制将一个活动涉及的所有操作纳入到一个不可分割的执行单元，组成事务的所有操作只有在所有操作均能正常执行的情况下方能提交，只要其中任一操作执行失败，都将导致整个事务的回滚。
### 1.1 1、ACID理论
- **原子性（Atomicity）** 所有操作，要么全部完成，要么全部不完成
- **一致性（Consistency）** 在事务开始之前和事务结束以后，数据库数据的一致性约束没有被破坏。 不能说凭空多了 100 块钱
- **隔离性（Isolation）** 不受未提交事务的影响。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。
- **持久性（Durability**   事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。
### 1.2 2、CAP 理论
> 一个分布式系统最多只能同时满足`一致性（Consistency）`、`可用性（Availability）`和`分区容忍性（Partition Tolerance）`这三项中的两项。
- **_一致性_**   指“所有节点同时看到相同的数据”，即更新操作成功并返回客户端完成后，所有节点在同一时间的数据完全一致，等同于所有节点拥有数据的最新版本。
- **_可用性_**   指“任何时候，读写都是成功的”，即服务一直可用，而且是正常响应时间。
- **_分区容忍性_** 指“当部分节点出现消息丢失或者分区故障的时候，分布式系统仍然能够继续运行”，即系统容忍网络出现分区，并且在遇到某节点或网络分区之间网络不可达的情况下，仍然能够对外提供满足一致性和可用性的服务。
- 其中，`P 是确定的`，因为网络断开是客观存在的，因此就是选择 **AP 架构**还是 **CP 架构**的问题，实现更好的**一致性和可用性**。

	CP、AP 架构的取舍案例。Zookeeper用来解决分布式集群中应用系统的协调和一致性问题，因此是 CP。 Eureka等服务发现组件是为了保证可用性，因此是 AP。

### 1.3 3、Base 理论
> BASE是 `Basically Available(基本可用）`、`Soft state(软状态）`和 `Eventually consistent(最终一致性）`三个短语的简写。核心思想是`最终一致性`。
- _基本可用_： 允许损失部分可用性，延长响应时间，降级服务，限流等时段。
- _软状态_： 允许系统在多个不同节点的数据副本存在数据延时。
- _最终一致性_： 数据不能一直处于软状态，在一个时间期限后保证所有副本的数据一致性。
- Base 是对 CAP 的实际应用，放弃强一致性，实现基本可用。

## 2 二、分布式事务解决方案
> 两阶段和三阶段提交协议、 TCC 分段提交，和基于消息队列的最终一致性设计。
### 2.1 1、2PC 两阶段提交
> Two-phase Commit Protocol  一致性、中心化的原子提交协议
- **提交请求阶段**：协调者将通知事务参与者准备提交事务，然后进入表决过程。在表决过程中，参与者将告知协调者自己的决策。
- **提交阶段**：协调者基于投票结果进行决策，所有参与者同意则提交事务。
- **问题**：资源阻塞，协调者单点故障，通知丢失造成数据不一致。
### 2.2 2、3PC 三阶段提交
> 在 2PC 之上扩展的提交协议，主要是为了解决两阶段提交协议的阻塞问题，从原来的两个阶段扩展为三个阶段，增加了超时机制。
- CanCommit 阶段：协调者向参与者发送 Can-Commit 请求
- PreCommit 阶段：协调者发送预提交请求，全部通过则进入 Prepared 阶段。
- DoCommit 阶段：进行事务提交和没收到通知后进行超时提交。
- 优点和问题：引入超时机制和预提交阶段，保证在最后提交前各节点状态一致
### 2.3 3、TCC 分段提交
> 分布式事务的处理模型，将事务过程拆分为 Try、Confirm、Cancel 三个步骤，在保证强一致性的同时，最大限度提高系统的可伸缩性与可用性。
- **_Try 阶段_**：先对资源进行锁定，资源处于中间态但不处于最终态
- **_Confirm 或 Cancel 阶段_**：在 Try 操作的基础上，真正提交这次修改操作还是回滚这次变更操作
- `事务协调器 TX Manager`：负责统筹分布式事务的执行，串联 Try -> Confirm/Cancel 的两阶段流程. 在第一阶段中批量调用 TCC Component 的 Try 接口，根据其结果，决定第二阶段是批量调用 TCC Component 的 Confirm 接口还是 Cancel 接口
### 2.4 4、基于消息补偿的最终一致性
> 具体实现上，基于消息补偿的一致性主要有本地消息表和第三方可靠消息队列等。
#### 2.4.1 基于 MQ 实现分布式事务
- MQ可以保证至少被消费一次，但是不能解决消息的重复性问题
- 消费者需要基于消息的唯一键执行幂等去重操作
## 3 三、Paxos 算法
### 3.1 1、Quorum 机制
>在 N 个副本中，一次更新成功的如果有 W 个，那么我在读取数据时是要从大于 N－W 个副本中读取，这样就能至少读到一个更新的数据了。
	WARO：全部更新完成才能写，保证所有副本一致
- 定义： 限定一次最少要读的副本数，如共 N 个副本，更新了 W 个，则要读取 N-W+1 个，保证读的数据是最新的。
- 需要配合版本号机制来确认。
### 3.2 2、Paxos 节点构成
	角色有三种，一个节点可以同时成为这三者
- 提案者（Proposer）：提出议案value，比如修改某个变量，一轮只批准一个 value。
- 批准者 （Acceptor）：value 超过半数（N/2+1）的 Acceptor 批准后才能通过
- 学习者（Learner）： 学习被批准的 value，参考 Quorum机制，至少读 N/2+1 个 Accpetor来学习到通过的 value。
- Client 产生议题者
### 3.3 3、选举过程
- 准备阶段：Proposer生成唯一的 ProposalID，发送 Prepare 请求。Acceptor 收到后，本地持久化并返回已经接收的提案。
- 选举阶段：
	- Proposer 发送 Accept，回复大于一半，发出accept 请求，并带上自己指定的 value。
	- Acceptor 应答 Accept，回复提交结果。
	- Proposer 统计投票，过半数回复成功，广播结果。失败则回到准备阶段。
## 4 四、Raft 算法
### 4.1 1、 概念介绍
> 使用了分治思想把算法流程分为三个子问题：选举（Leader election）、日志复制（Log replication）、安全性（Safety）三个子问题
- 节点被分为 Leader Follower Cabdidate 三种角色：
	- **Leader**：处理与客户端的交互和与 follower 的日志复制等，一般只有一个 Leader；
	- **Follower**：被动学习 Leader 的日志同步，同时也会在 leader 超时后转变为 Candidate 参与竞选；
	- **Candidate**：在竞选期间参与竞选；
- **_Term_**：**Raft 算法将时间划分成为任意不同长度的任期（term）**。任期用连续的数字进行表示。**每一个任期的开始都是一次选举（election），一个或多个候选人会试图成为领导人**。如果一个候选人赢得了选举，它就会在该任期的剩余时间担任领导人。在某些情况下，选票会被瓜分，有可能没有选出领导人，那么，将会开始另一个任期，并且立刻开始下一次选举。**Raft 算法保证在给定的一个任期最多只有一个领导人**。
- **_随机超时时间_**：Follower 节点每次收到 Leader 的心跳请求后，会设置一个随机的，区间位于（150ms, 300ms)的超时时间。如果超过超时时间，还没有收到 Leader 的下一条请求，则认为 Leader 过期/故障了。
- **心跳续命**：Leader 在当选期间，会以一定时间间隔向其他节点发送心跳请求，以维护自己的 Leader 地位。
- Raft 算法中服务器节点之间通信使用远程过程调用（RPC）
	- RequestVote RPC：候选人在选举期间发起。
	- AppendEntries RPC：领导人发起的一种心跳机制，复制日志也在该命令中完成。
	- installSnapshot RPC: 领导者使用该RPC来发送快照给太落后的追随者。
### 4.2 2、协议流程
- _选举流程_
	- 当某个 follower 节点在超时时间内未收到 Leader 的请求，将发起选举， 从一个 Follower 变成 Candidate
	- 如果一个 Candidate 收到了超过半数的投票，则该节点晋升为 Leader，会广播给所有节点；开始进行日志同步、处理客户端请求等
	- term用来保证请求的合法性
- _日志复制_
	- `复制状态机`：不同节点从相同的初始状态出发，执行相同顺序的输入指令集后，会得到相同的结束状态。
	- 节点初始化后具有相同初始状态，将一个客户端请求（command）封装到一个` log entry` 中。Leader 负责将这些 log entries 复制到所有的 Follower 节点，然后节点按照相同的顺序应用 commands，达到`最终的一致状态`
	- Leader执行请求过程：
		- 本地追加日志信息；
		- 并行发出 AppendEntries RPC 请求；
		- 等待大多数 Follower 的回应。收到查过半数节点的成功提交回应，代表该日志被复制到了大多数节点中(committed)；
		- 在状态机上执行 entry command。既将该日志应用到状态机，真正影响到节点状态(applied)；
		- 回应 Client 执行结果；
		- 确认 Follower 也执行了这条 command；如果 Follower 崩溃、运行缓慢或者网络丢包，Leader 将无限期地重试 AppendEntries RPC，直到所有 Followers 应用了所有日志条目。
### 4.3 3、安全性及约束
#### 4.3.1 选举安全性
- 任一任期内最多一个 leader 被选出，有多余的 Leader就是脑裂了
	- 一个节点某一任期内最多只能投一票；而节点 B 的 term 必须比 A 的新，A 才能给 B 投票
	- 只有获得多数投票的节点才会成为 leader
#### 4.3.2 日志 append only
- leader 在某一 term 的任一位置只会创建一个 log entry，且 log entry 是 append-only
- 一致性检查，请求中会包含最新 log entry 的前一个 log 的 term 和 index，如果 follower 在对应的 term index 找不到日志就会重新进行同步
#### 4.3.3 日志匹配特性
- 如果两个节点上的某个 log entry 的 log index 相同且 term 相同，那么在该 index 之前的所有 log entry 应该都是相同的。
#### 4.3.4 Leader 完备性
- 被选举人必须比自己知道的更多（比较 term 、log index）
#### 4.3.5 状态机安全性
- 状态机安全性由日志的一致来保证。在算法中，一个日志被复制到多数节点才算 committed， 如果一个 log entry 在某个任期被提交（committed），那么这条日志一定会出现在所有更高 term 的 leader 的日志里面



## 5 五、ZooKeeper
	ZooKeeper 提供了一个类似于 Linux 文件系统的数据模型，和基于 Watcher 机制的分布式事件通知。
### 5.1 1、Zab 一致性协议
	ZooKeeper Atomic Broadcast，ZooKeeper 原子广播协议，保证分布式事务的最终一致性。

	具体实现
- 消息广播阶段，Leader 节点接受事务提交并将请求广播给 Follower 节点，根据反馈决定是否 Commit。
- 崩溃恢复阶段，Leader 宕机，重新进行 Leader 选举并进行数据同步。
- Zxid: 事务编号，有一个新的 Leader 选举出现时，就会从这个 Leader 服务器上取出其本地日志中最大事务的 Zxid，并从中读取 epoch 值，然后加 1，以此作为新的周期 ID。高 32 位代表了每代 Leader 的唯一性，低 32 位则代表了每代 Leader 中事务的唯一性。

## 6 六、分布式锁
	分布式锁的目的是保证在分布式部署的应用集群中，多个服务在请求同一个方法或者同一个业务操作的情况下，对应业务逻辑只能被一台机器上的一个线程执行，避免出现并发问题。
- **互斥性**: 任意时刻，只有一个客户端能持有锁。
- **锁超时释放**：持有锁超时，可以释放，防止不必要的资源浪费，也可以防止死锁。
- **可重入性**:一个线程如果获取了锁之后,可以再次对其请求加锁。
- **高性能和高可用**：加锁和解锁需要开销尽可能低，同时也要保证高可用，避免分布式锁失效。
- **安全性**：锁只能被持有的客户端删除，不能被其他客户端删除
### 6.1 1、基于数据库
	基于关系型数据库实现分布式锁，是依赖数据库的唯一性来实现资源锁定，比如主键和唯一索引等。
	问题：单点故障，超时无法失效，不可重入，不能阻塞。
### 6.2 2、基于缓存 Redis
- SETNX + EXPIRE：不是原子操作
- SETNX + value值是(系统时间+过期时间)：要求时间同步，没有持有者的标识
-  使用Lua脚本(包含SETNX + EXPIRE两条指令)：原子性
-  SET的扩展命令（SET EX PX NX）：误删，过期还没执行完
-  SET EX PX NX + 校验唯一随机值,再删除：设置 value 标记线程
-  Redisson框架：开启一个定时守护线程，延长过期时间
- setnx 是「set if not exists」

> 高可用，Redlock算法，基于 N 个完全独立的 Redis 节点，一般是大于 3 的奇数个
- 客户端记录当前系统时间，以毫秒为单位；
- 依次尝试从 5 个 Redis 实例中，使用相同的 key 获取锁，当向 Redis 请求获取锁时，客户端应该设置一个网络连接和响应超时时间，超时时间应该小于锁的失效时间，避免因为网络故障出现的问题；
- 客户端使用当前时间减去开始获取锁时间就得到了获取锁使用的时间，当且仅当从半数以上的 Redis 节点获取到锁，并且当使用的时间小于锁失效时间时，锁才算获取成功；
- 如果获取到了锁，key 的真正有效时间等于有效时间减去获取锁所使用的时间，减少超时的几率；
- 如果获取锁失败，客户端应该在所有的 Redis 实例上进行解锁，即使是上一步操作请求失败的节点，防止因为服务端响应消息丢失，但是实际数据添加成功导致的不一致。
 
### 6.3 3、基于Zookeeper
- 利用 ZooKeeper 支持临时顺序节点的特性，实现分布式锁。
- 当客户端对某个方法加锁时，在 ZooKeeper 中该方法对应的指定节点目录下，生成一个唯一的临时有序节点。
- 判断是否获取锁，只需要判断持有的节点是否是有序节点中序号最小的一个，当释放锁的时候，将这个临时节点删除即可，这种方式可以避免服务宕机导致的锁无法释放而产生的死锁问题。
	- 客户端连接 ZooKeeper，并在 /lock 下创建临时有序子节点，第一个客户端对应的子节点为 /lock/lock01/00000001，第二个为 /lock/lock01/00000002；
	- 其他客户端获取 /lock01 下的子节点列表，判断自己创建的子节点是否为当前列表中序号最小的子节点；
	- 如果是则认为获得锁，执行业务代码，否则通过 watch 事件监听 /lock01 的子节点变更消息，获得变更通知后重复此步骤直至获得锁；
	- 完成业务流程后，删除对应的子节点，释放分布式锁。