---
title: "[DDIA] 批处理"
date: 2023-08-10
tags:
  - DDIA
categories:
  - 阅读笔记
  - DDIA
published: true
toc: "true"
comments: "true"
description:
---
>批处理系统通常会接受大量数据作为输入，然后基于这些数据执行任务，进而产生一些数据作为输出。

<!--more-->

web 服务和日趋增长的基于 HTTP/REST 的 API，让请求/应答风格的交互如此普遍，以至于我们理所当然的认为系统就应该长这样。但须知，这并非构建系统的唯一方式，其他方法也各有其应用场景。我们来对下面三种类型系统进行考察：

- **服务（在线系统，online systems）** 服务（service）类型的系统会等待客户端发来的请求或指令。当收到一个请求时，服务会试图尽快的处理它，然后将返回应答。**响应时间**通常是衡量一个服务性能的最主要指标，且**可用性**通常很重要（如果客户端不能够触达服务，则用户可能会收到一条报错消息）。之前章节我们主要在讨论此类系统。
- **批处理系统（离线系统，offline systems）** 一个批处理系统通常会接受大量数据作为输入，然后在这批数据上跑**任务**（job），进而产生一些数据作为输出。任务通常会运行一段时间（从数分钟到数天不等），因此一般来说没有用户会死等任务结束。相反，批处理任务通常会周期性的执行（例如，每天一次）。**吞吐量**（throughput，处理单位数据量所耗费的时间）通常是衡量批处理任务最主要指标。本章会主要围绕该类型系统进行讨论。
- **流式系统（近实时系统，near-real-time systems）** 流式处理介于在线处理和离线处理（批处理）之间（因此也被称为**近实时**，near-real-time，或者**准在线处理**，nearline processing）。和批处理系统一样，流式处理系统接受输入，产生一些输出（而不是对请求做出响应，因此更像批处理而非服务）。然而，一个流式任务通常会在事件产生不久后就对其进行处理，与之相对，一个批处理任务通常会**攒够一定尺寸**的输入数据才会进行处理。这种区别让流式处理系统比同样功能的批处理系统具有更低的延迟。由于流式处理基于批处理，因此我们下一章再讨论它。

批处理是寻求构建**可靠的、可扩展的、可维护**的应用的重要组成部分。


## 1 使用Unix工具进行批处理

有一个 web 服务器，并且当有请求进来时，服务器就会向日志文件中追加一行日志：
```cpp
216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1" 200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

在 2015 年的 2 月 27 号，UTC 时间 ，服务器从 IP 为 216.58.210.78 的客户端收到了一条请求，请求路径为 `/css/typography.css`。该用户没有经过认证，因此用户位置显示了一个连字符（-）。响应状态码是 200（即，该请求成功了），响应大小是 3377 字节。web 浏览器是 Chrome 49，由于该资源在 [http://martin.kleppmann.com/](http://martin.kleppmann.com/) 网站中被引用，因此浏览器加载了该 CSS 文件。

### 1.1 简单的日志分析
获取**网站上访问频次最高的五个页面**，则可以在 Unix Shell 中输入：
```bash
cat /var/log/nginx/access.log | #(1)
      awk '{print $7}' |        #(2)
      sort             |        #(3)
      uniq -c          |        #(4)
      sort -r -n       |        #(5)
      head -n 5                 #(6)
```

每一行作用如下：
1. 读取给定日志文件
2. 将每一行按空格分成多个字段，然后取出第七个，即我们关心的 URL 字段。在上面的例子中，即：`/css/typography.css`
3. 按字符序对所有 url 进行排序。如果某个 url 出现了 n 次，则排序后他们会连着出现 n 次。
4. `uniq` 命令会将输入中相邻的重复行过滤掉。`-c` 选项告诉命令输出一个计数：对于每个 URL，输出其重复的次数。
5. 第二个 `sort` 命令会按每行起始数字进行排序（`-n`），即按请求次数多少进行排序。`-r` 的意思是按出现次数降序排序，不加该参数默认是升序的。
6. 最后，`head` 命令会只输出前 5 行，丢弃其他多余输入。
比如，如果你想**在输出中跳过 CSS 文件**，可以将 awk 的参数改成 `'$7 !~ /\.css$/ {print $7}'` 。如果你想**统计最常访问的 IP 数**而非访问网页，则可以将 awk 的参数变为 `'{print $1}'`。如此种种。


#### 1.1.1 排序 vs 内存聚合

Ruby 脚本在内存中保存了 URL 的**哈希表**，记录每个 URL 到其出现次数的映射。Unix 管道例子中并没有这样一个哈希表。作为替代，它将所有 URL 进行**排序**，从而让所有相同的 URL 聚集到一块，从而对 URL 出现次数进行统计。

如果工作集足够小，则基于内存的哈希表能够很好地工作
但如果，任务的工作集大于可用内存，则排序方式更有优势，因为能够充分利用磁盘空间。

### 1.2 Unix 哲学

1. **每一个程序专注干一件小事**。在想做一个新任务时，新造一个轮子，而非向已有的程序中增加新的“功能”。
2. **每个程序的输出成为其他程序（即便下一个程序还没有确定）的输入**。不要在输出中混入无关信息（比如在数据中混入日志信息），避免使用严格的列式数据（数据要面向行，以行为最小粒度？）或者二进制数据格式。不要使用**交互式输入**。
3. **尽快的设计和构建软件**，即便复杂如操作系统，也最好在几周内完成（译注：这里翻译稍微有些歧义，即到底是尽快迭代还是尽早让用户试用，当然他们最终思想差不多，即构造最小可用模型，试用-迭代）。对于丑陋部分，不要犹豫，立即推倒重构。
4. （Q：unskilled help是指？这一条没太理解）**相比不成熟的帮助，更倾向于使用工具完成编程任务**，即使可能会进行反复构建相似的工具，并且在用完之后大部分工具就再也不会用到。
尽可能自动化、快速原型验证、小步增量迭代、易于实验测试，将大型工程拆解成一组易于管理的模块

#### 1.2.1 统一的接口
如果你想让任意程序的输出能接到任意程序的输入上，则意味着**所有**这些程序必须使用同样的**输入输出**接口。
在 Unix 中，这种接口是**文件**（a file，更准确的说，是文件描述符，file descriptor）。文件本质上是一种**有序的字节序列**。
`awk`，`sort`，`uniq` 和 `head` 都将其输入文件视为由 `\n`（换行符，ASCII 码是 0x0A）分割的一系列记录。

## 2 MapReduce和分布式文件系统
一个 MapReduce 任务就像一个 Unix 进程：**接受一到多个输入，产生一到多个输出**。
和 Unix 工具一样，执行一个 MapReduce 任务**不会修改输入文件**，并且除了产生输出**没有其他的副作用**。输出文件都是**单次写入、顺序追加**而成（即 ，一旦文件写完，就不会再有任何改动）。
相比 Unix 工具使用 stdin 和 stdout 作为输入和输出，MapReduce 任务的输入和输出都是**分布式文件系统上的文件**。在 Hadoop 的 MapReduce 实现中，该文件系统被称为 HDFS（Hadoop Distributed File System），是谷歌文件系统（GFS，Google File System）的一个开源实现。

HDFS 基本设计理念是：**shared-nothing** （机器间不共享任何特殊硬件，纯通过网络来通信）架构。HDFS 和**对象存储**不同点之一是，HDFS 能够将计算**就近的**调度到存储所在的机器上（调度亲和性，本质原因在于计算和存储在同一个集群，有好处也有劣势），但对象存储会将存储和计算分离。

HDFS 由一组运行在每个主机上的**守护进程**（Daemon Process）组成，对外暴露网络接口，以使其他的节点可以访问存储于本机的数据文件（假设数据中心中的通用机器节点上都附有一定数量的磁盘）。一个叫做 **NameNode** 的**中心节点**会保存文件块和其所在机器的映射（也即文件块的 placement 信息）。因此，HDFS 可以利用所有运行有守护进程的机器上（**DataNode**）存储空间，在逻辑上对外提供单一且巨大的文件系统抽象。

### 2.1 MapReduce 任务执行
1. **读取一组输入文件，将其切分为记录（records）**。在网站服务器日志的例子中，每个记录就是日志中的一行（即，使用 \n 作为记录分隔符）
2. **调用 Mapper 函数从每个记录中抽取 key 和 value**。在之前的例子中，mapper 函数是 `awk '{print $7}'` ：抽取 URL（$7）作为 key，value 留空。
3. **将所有的 key-value 对按 key 进行排序**。在前面例子中，该环节由 sort 承担。
4. **调用 Reducer 函数对排好序的 kv 列表迭代处理**。如果某个 key 出现了多次，排序环节会让其在在列表中集中到一块，因此可以在不在内存中保存过多状态的的情况下，对具有相同 key 的数据进行汇总处理。在前面例子中，reducer 对应命令 `uniq -c` ，功能是对所有具有相同 key 的记录值进行计数。
这四个步骤（split-map-sort-reduce）可以通过一个 MapReduce 任务来实现。你可以在步骤 2 （map）和步骤 4（reduce）编写代码来自定义数据处理逻辑。步骤 1 （将文件拆分成记录）由**输入格式解析器**（input format parser）来完成。步骤 3，排序阶段，由 MapReduce 框架**隐式完成**，所有 Mapper 的输出在给到 Reducer 前，框架都会对其进行排序。

需要实现两个回调函数：mapper 和 reducer，其行为如下：
- **Mapper** 对于每个输入**记录**都会调用一次 Mapper 函数，其任务是从记录中抽取 key 和 value。对于每一个输入记录，都有可能产生任意数量（包括 0 个）的 kv 对。框架不会保存任何**跨记录的状态**，因此每个记录都可以独立的被处理（即 Mapper 可以进行任意并发的运行）。
- **Reducer** MapReduce 框架会拿到 Mapper 输出的 kv 对，通过排序将具有相同 key 的 value 聚集到一块，以迭代器的形式给到 Reducer 函数。reducer 会继续输出一组新的记录（如 URL 的出现频次）。

#### 2.1.1 MapReduce 的分布式执行
与 Unix 工具流水线的相比，MapReduce 的最大区别在于可以在**多台机器上**进行分布式的执行，但并不需要用户显式地写处理并行的代码。mapper 和 Reducer 函数每次只处理一个记录；他们不必关心输入从哪里来，输出要到哪里去，框架会处理分布式系统所带来的的复杂度

每个输入文件通常有数百 M，每个输入通常有多个副本，分散在多个机器上。MapReduce 的调度器（图中没有显示）在调度时，会在这多个副本所在机器上选择一个具有足够内存和 CPU 资源运行该 Mapper 任务的机器，将 map 任务调度过去。这个策略也被称为：**将计算调度到数据上**。从而省去在网络中拷贝数据的环节，提高了局部性，减少了网络带宽消耗。

多数情况下，**应用层的代码**通常不会存在于 map 任务调度到的机器上。因此，MapReduce 框架首先会将用户代码（如对于 Java 来说就是 Jar 包）**序列化后**复制过去。然后在对应机器上，动态加载这些代码，继而执行 map 任务。读取输入文件，逐个解析数据记录（record），传给 Mapper 回调函数执行。每个 Mapper 会产生一组 key-value 对。

reduce 侧的计算也是分片的。对于 MapReduce 任务来说，**map 任务的数量**，取决于该任务的输入文件数（或者文件 block 数）的数量；**但 reduce 任务的多少**，可以由用户显式的配置（可以不同于 map 任务的数量）。为了保证所有具有相同 key 的 kv 对被同一个 Reducer 函数处理，框架会使用哈希函数，将所有 Mapper 的输出的 kv 对进行分桶（桶的数量就是 Reducer 的数量），进而路由到对应的 Reducer 函数。

根据 MapReduce 的设定，reducer 接受的 kv 对需要是有序的，但任何传统的排序算法都无法在单机上对如此大尺度的数据进行排序。为了解决这个问题，mapper 和 Reducer 间的**排序被分成多个阶段**。

首先，每个 map 任务在输出时，会先将所有输出哈希后分片（一个分片对应一个 reducer），然后在每个分片内对输出进行排序。由于每个分片的数据量仍然可能很大，因此使用外排算法。

当某个 Mapper 任务读取结束，并将输出排好了序，MapReduce 调度器就会通知所有 reducers 来该 Mapper 机器上拉取各自对应的输出。最终，每个 Reducer 会去所有 Mapper 上**拉取**一遍其对应分片数据数据。这里有个推还是拉的设计权衡，拉的好处在于 reducuer 失败后，可以很方便地进行重试，再次拉取计算即可。

这个**分片**（partitioning by reducer）-**排序**（sorting）-**复制**（coping）过程也被称为**数据重排**（**shuffle**，虽然英文是洗牌的意思，但该过程并没有任何随机性，都是确定的）。

框架会在 Reducer 处将所有从 Mapper 处拿来的 kv 文件进行归并排序，然后在所有数据拉取完毕后，将排好序数据送给 reducer。这样一来，不同 Mapper 产生的具有相同 key 的记录就会被聚集到一块。

总结来说，map 和 reduce 间的排序分为**两个阶段**：

1. 在每个 Mapper 上对**输出**分片后各自排序。
2. 在每个 Reducer 上对**输入**（有序文件）进行归并排序。
Reducer 在调用时会传入一个 key 一个 Iterator（迭代器），使用该迭代器能够访问所有具有相同 key 的记录（极端情况下，内存可能放不下这些记录，因此是给一个迭代器，而非内存数组）。reducer 函数可以使用任意的逻辑对这些记录进行处理，并可以产生任意数量的输出。这些输出最终会被写到分布式文件系统中的文件里（通常该输出文件会在 Reducer 机器上放一个副本，在另外一些机器上放其他副本）。

#### 2.1.2 MapReduce 工作流
将多个 MapReduce 首尾相接（前面任务的输出作为后面任务的输入）地串成**工作流**（workflow）极为常见。Hadoop MapReduce 框架本身没有提供任何关于工作流的支持，因此通常依赖文件夹名进行**隐式的链式调用**：

1. 第一个 MapReduce 任务将其输出写入特定的文件夹。
2. 第二个 MapReduce 任务读取这些文件夹中文件作为输入。

仅当一个任务完全成功的执行后，其输出才被认为是有效的（也即，MapReduce 任务会丢掉失败任务的不完整输出）。因此，工作流中的任务只有在前一个任务成功结束后才能启动——即，前驱任务必须**成功地**将输出写入到对应文件夹中。为了处理多个任务间执行的依赖关系（比如 DAG 依赖），人们开发了很多针对 Hadoop的工作流调度框架，如 Oozie，Azkaban，Luigi，Airflow 和 Pinball。

### 2.2 Reduce 侧的 Join 和 Group

在很多数据集中，一个记录和其他记录有**关联**（association）是一个很常见的现象：关系模型中的**外键**（foreign key），文档模型中的**文档引用**（document reference），图模型中的**边**（edge）。在代码需要访问有关联的双方记录（引用记录和被引用记录）时，Join 是必须的

当一个 MapReduce 任务拿到一组输入文件时，会读取文件中的所有内容；在数据库中，这种操作称为**全表扫描**（full table scan）。如果你进项访问一小部分记录，相比索引查找，全表扫描操作会非常的重。

#### 2.2.1 基于排序-合并的 Join

 Mapper 的职责：从所有输入记录中提取 key 和 value。
 
 当 MapReduce 框架将所有 Mapper 的输出按照 key（也就是用户 ID）进行排序后，所有具有同样的用户 ID 的记录就会聚集到一块，作为输入给到 reducer。MapReduce 任务甚至可以将输出进行特殊组织，以使 Reducer 先看到同一个用户的资料信息，再看到其行为信息——这种技术也被称为**二级排序**（secondary sort，使用多个字段进行排序）。

在此基础上，reducer 可以进行轻松的进行 join：reducer 函数会在每一个用户 ID 上进行调用，由于使用了二级排序，reducer 会先看到该用户的资料信息。在实现 Reducer 时，可以首先将用户**资料信息**（比如生日）保存在局部变量里，然后对其所有**行为信息**进行迭代，提取相关信息，输出 <viewed-url, viewed age in years> kv 对。之后可以再接一个 MapReduce 任务，对每个 url 访问用户的年龄分布进行统计，并按年龄段进行聚集。

由于 Reducer 会在单个函数里处理所有同一个 user ID 的记录，因此一次只需要在内存中保存一个用户的资料信息，并且不用进行任何网络请求。这种算法也被称为**基于排序和归并的连接**（sort-merge join），由于 Mapper 的输出是按 key 有序的，则 reducers 可将来自多方的同一个 key 的输入轻松的进行合并。


#### 2.2.2 将相关数据聚到一块

在排序-归并 join 中，mappers 和排序会确保同一个用户 id 所有用于 join 必要输入会被放到一起：**即作为一个输入给到某次 Reducer 中**。预先让所有相关数据聚集到一起，可以让 Reducer 逻辑非常简单，并且可以仅使用单个线程，就能进行高吞吐、低耗存地执行。

我们可以从另外一种角度来理解这种架构：mapper **发消息**给 reducer。当某个 Mapper 发出一个 key-value 对时，**key 是投递地址，value 就是要投递的内容**。尽管 key 在物理上仅是一个任意的字符串（而非像网络中的 IP 和端口号那样真的网络地址），但在逻辑上充当**地址**的作用：所有具有相同 key 的 kv 对都会被投递到同一个目的地（某个 Reducer 的调用处）。

MapReduce 编程模型，可以将计算的**物理拓扑**（将数据放到合适的机器上）与**应用逻辑**（当有了数据后就进行处理）**解耦**开来。这种解耦与数据库形成对比——在使用数据库的场景中，进行数据库连接（物理）通常藏在应用代码（逻辑）深处。由于 MapReduce 框架会处理所有网络通信细节，它也会让应用层代码免于关心**部分失败**（partial failure），如某些节点宕机：MapReduce 框架会透明的（应用代码无感）的对失败的子任务进行重试，而不会影响应用逻辑。

#### 2.2.3 Group By

将所有记录按某些 key 进行**分组**（对应 SQL 中的 `GROUP BY` 子句）。首先将具有相同 key 的所有记录被分到一组，然后对这些分组分别执行某些**聚集**操作（aggregation）

使用 MapReduce 实现 Group By 语义，最简单的方法是在 Mapper 中**抽取 key 为待分组的 key**。MapReduce 框架就会按照这些 key 将所有 Mapper 的输出记录进行分区和排序，然后按 key 聚集给到 reducer。本质上，使用 MapReduce 来实现 group 和 join ，逻辑是极为相似的。

分组的另外一个使用场景是：收集某个用户会话中的所有用户活动——也称为**会话化**（sessionization）。例如，可以用来对比用户对于新老版本网站的分别购买意愿（A/B 测试）或者统计某些市场推广活动是否起作用。

假设你的 web 服务架设在多台服务器上，则某个特定用户的活动日志大概率会分散在不同服务器上。这时，你可以实现一个会话化的 MapReduce 程序，使用会话 cookie、用户 ID或者其他类似的 ID 作为分组 key，以将相**同用户**的所有活动记录聚集到一块、并将**不同用户**分散到多个分区进行处理。

#### 2.2.4 处理偏斜（skew）

如果某个 key 的数据量超级大，则“将相同 key 的数据聚集到一块” 的模型将不再适用。例如，在社交网络中，绝大多数的人都只会连接到较少的其他人，但数量较少的名人会有高达数百万的关注者。数据库中这种不成比例的记录常被称为**关键对象**（_linchpin objects_ ）或者**热键**（_hot keys_）。
在单个 Reducer 中收集处理名人（celebrity）所有的活动事件（比如他们发布信息的回复），可能会造成严重的**数据倾斜**（**skew**，有时也被称为热点，hot spots）——即，一个 Reducer 处理的数据量远超其他。由于只有其所属的所有 mappers 和 reducers 执行完时，该 MapReduce 任务才算完成，该 MapReduce 之后的任何任务都需要等待最慢的 Reducer （长尾任务）完成后才能启动。


如果某个 join 的输入存在热点数据，你可以借助一些算法来进行缓解。例如，Pig 中的偏斜 join（skewed join）方法会事先对所有 key 的分布进行**采样**，以探测是否有热点 key。然后，在执行真正的 Join 时，对于 Join 有热点 key 的这一测，mapper 会将含有热点 key 的记录发送到多个 reducer（每次随机挑选一个，相比之下，常规的 MapReduce 只会根据 key 的哈希确定性的选择一个 reducer）；对于 Join 的另一侧输入，所有包含热点 key 的相关记录需要每个给每个具有该 key 的 Reducer 都发一份。

该技术将处理热点 key 的工作分摊到多个 Reducer 上，从而可以让其更好的并行，当然代价就是需要将 join 的非热点侧的数据冗余多份。Crunch 中的**分片连接**（shared join）也使用类似的技术，但需要**显式地指定**热点 key，而非通过采样来**自动获取**。

当对热点 key 进行分组聚集（group）时，可以将分组过程拆成**两个阶段**，即使用两个相接的 MapReduce。第一个 MapReduce 会将记录随机得发给不同的 reducer，则每个 Reducer 会对热点 key 的一个子集执行分组操作，并且产生一个更为紧凑的**聚合值**（aggregated value，如 count，sum，max 等等）。第二个 MapReduce 操作会将第一阶段中 MapReduce 产生的同一个 key 的多个聚合值进行真正的归并。总结来说，就是第一阶段进行**预分组，减小数据量**；第二阶段真正的全局分组，可以想象这种方式，要求聚合操作满足**交换律和结合律**。

### 2.3 Map 侧的连接

上一节讲到的 join 算法是在 reduce 阶段真正执行的 join 逻辑，因此也被称为 **reduce 侧连接**（_reduce-side join_）。其中，mapper 仅扮演准备数据的角色：从每个输入记录中提取 key 和 value，并且将每个 kv 对发给合适的 Reducer 分区，并将其进行排序。

reduce 侧的连接的好处在于，你不需要对输入数据有任何的假设：不管输入数据具有怎样的属性和结构，mappers 都可以进行合适的预处理后送给 reducers 进行连接。然而，缺点在于排序、复制到 reducers、将 Reducer 的输入进行合并等过程代价十分高昂。根据可用内存缓存大小不同，数据在流经 MapReduce 中各阶段时可能会被写入多次（写放大）。

但如果，输入数据满足某种假设，就可以利用所谓的 **map 侧连接**（map-side join）进行更快的连接。这种方式利用了一种简化过的 MapReduce 任务，去掉了 reducer，从而也去掉了对 Mapper 输出的排序阶段。此时，每个 Mapper 只需要从分布式文件系统中的输入文件块中读取记录、处理、并将输出写回到文件系统，即可。

#### 2.3.1 广播哈希连接

使用 map 侧连接的一个最常见的场景是一个大数据集和一个小数据集进行连接时。此种情况下，小数据集需要小到能全部装进 Mapper 进程所在机器的内存。
但仍然会有多个 Mapper 任务：join 的**大数据量输入侧** 每个文件块一个 mapper。其中 MapReduce 任务中的每个 Mapper 都会将小输入侧的数据全部加载进内存。

称为**广播哈希连接**（broadcast hash joins）：
1. **广播（broadcast）**：处理大数据侧每个分片的 Mapper 都会将小数据侧数据全部载入内存。从另外一个角度理解，就是将小数据集**广播到了**所有相关 Mapper 机器上。
2. **哈希（hash）**：即在将小数据集在内存中组织为哈希表。

#### 2.3.2 分区哈希连接
如果待 join 的多个输入，能够以同样的方式进行分区，则每个分区在处理时可以独立地进行 join。

如果分区方式正确，则所有需要连接的双方都会落到同一个分区内，因此每个 Mapper 只需要读取一个分区就可以获取待连接双方的所有记录。这样做的好处是，每个 Mapper 所需构建哈希表的数据集要小很多。

在 Hive 中，分区哈希连接也被称为**分桶 map 侧连接**（ bucketed map join）。

#### 2.3.3 Map 侧合并连接

当 map 的输入数据集不仅以相同的方式分片过了，而且每个分片是**按该 key 有序的**。在这种情况下，是否有足够小的、能够载入内存的输入已经无关紧要，因为 Mapper 可以以类似普通 Reducer 的方式对输入数据进行**归并**：都以 key 递增（都递减也可以，取决于输入文件中 key 的顺序）的顺序，增量式（迭代式）的读取两个输入文件，对相同的 key 进行匹配连接。


### 2.4 批处理工作流的输出

对于数据库查询场景，我们会区分事务型处理场景（OLTP）和分析性场景（OLAP）

OLTP 场景下的查询通常只会涉及很小的一个数据子集，因此通常会使用索引加速查询，然后将结果**展示**给用户（例如，使用网页展示）。另一方面，分析型查询通常会扫描大量的数据记录，执行**分组**（grouping）和**聚集**（aggregating）等统计操作，然后以**报表**的形式呈现给用户。

一组 MapReduce 任务组成的执行流通常和用于分析型的 SQL 查询并不相同（参见 Hadoop 和分布式数据库的对比）。批处理的输出通常不是一个报表，而是**另外某种格式的数据**。

#### 2.4.1 构建查询索引
倒排索引是一个**词表**（the term dictionary），利用该词表，你可以针对关键词快速地查出对应**文档列表**（the postings list）。

在一个**固定文档集合**上构建全文索引，批处理非常合适且高效：
1. Mapper 会将文档集合按合适的方式进行分区
2. Reducer 会对每个分区构建索引
3. 最终将索引文件写回分布式文件系统

由于使用关键词进行索引查询是一种只读操作，因此，这些索引文件一旦构建完成，就是不可变的（immutable）。

如果被索引的文档集发生变动，一种应对策略是，定期针对所有文档重跑全量索引构建工作流（workflow），并在索引构建完时使用新的索引对旧的进行整体替换。如果两次构建之间，仅有一小部分文档发生了变动，则这种方法代价实在有点高。但也有优点，索引构建过程很好理解：**文档进去，索引出来**。

#### 2.4.2 以 KV 存储承接批处理输出

搜索索引只是批处理工作流一种可能的输出。批处理其他的用途还包括构建机器学习系统，如**分类器**（classifiers，如 垃圾邮件过滤，同义词检测，图片识别）和**推荐系统**（recommendation system，如你可能认识的人，可能感兴趣的产品或者相关的检索）。

在批处理任务内部生成全新的数据库，并将其以文件的形式写入分布式系统的文件夹中。一旦任务成功执行，这些数据文件就会称为**不可变的**（immutable），并且可以**批量加载**（bulk loading）进只处理只读请求的服务中。很多 KV 存储都支持使用 MapReduce 任务构建数据库文件，比如 Voldemort，Terrapin， ElephantDB 和 HBase bulk loading。另外 RocksDB 支持 ingest SST 文件，也是类似的情况。

**直接构建数据库底层文件**，就是一个 MapReduce 应用的绝佳案例：使用 Mapper 抽取 key，然后利用该 key 进行排序，已经**覆盖了**构建索引中的大部分流程。由于大部 KV 存储都是只读的（通过批处理任务一次写入后，即不可变），这些存储的底层数据结构可以设计的非常简单。

当数据加载进 Voldemort 时，服务器可以利用老文件**继续对外提供服务**，新文件会从分布式文件系统中拷贝的 Voldemort 服务本地。一旦拷贝完成，服务器可以立即将外部查询请求**原子地**切到新文件上。如果导入过程中发生了任何问题，也可以**快速地切回**，使用老文件提供服务。因为老文件是不可变的，且没有立即被删除。


#### 2.4.3 批处理输出的哲学

- **容忍人为错误**。如果你在代码中不小心引入了 bug，使得输出出错，你可以简单地将代码回滚到最近一个正确的版本，然后重新运行任务，则输出就会变正确。或者，更简单地，你可将之前正确的输出保存在其他的文件夹，然后在遇到问题时简单的切回去即可。使用读写事务的数据库是没法具有这种性质的：如果你部署了有 bug 的代码，并且因此往数据库中写入了错误的数据，回滚代码版本也并不能**修复这些损坏的数据**。（从有 bug 的代码中恢复，称为容忍人为错误，human fault tolerance）。这其实是通过牺牲空间换来的，也是经典的增量更新而非原地更新。
- **便于敏捷开发**。相比可能会造成不可逆损坏的环境，由于能够很方便地进行回滚，可以大大加快功能迭代的速度（因为不需要进行严密的测试即可上生产）。**最小化不可逆性**（_minimizing irreversibility_）的原则，有助于敏捷软件开发。
- **简单重试就可以容错**。如果某个 map 或者 reduce 任务失败了，MapReduce 框架会自动在相同输入上对其重新调度。如果失败是由代码 bug 引起的，在重试多次后（可以设置某个阈值），会最终引起任务失败；但如果失败是暂时的，该错误就能够被容忍。这种自动重试的机制之所以安全，是因为输入是不可变的，且失败子任务的输出会被自动抛弃。
- **数据复用**。同一个文件集能够作为不同任务的输入，包括用于计算指标的监控任务、评估任务的输出是否满足预期性质（如，和之前一个任务的比较并计算差异）。
- **逻辑布线分离**。和 Unix 工具一样，MapReduce 也将逻辑和接线分离（通过配置输入、输出文件夹），从而分拆复杂度并且提高代码复用度：一些团队可以专注于实现干好单件事的任务开发；另一些团队可以决定在哪里、在何时来组合跑这些代码。

### 2.5 对比 Hadoop 和分布式数据库

Hadoop 很像一个**分布式形态的 Unix**。其中，HDFS 对标 Unix 中的文件系统，MapReduce 类似于 Unix 进程的一个奇怪实现（在 map 阶段和 reduce 阶段间必须要进行排序）。

#### 2.5.1 存储类型更为多样

Hadoop 允许你以**任意格式**的数据灌入 HDFS，将如何处理的灵活性推到之后

Hadoop 经常用于 **ETL 处理**：将数据以某种原始的格式从事务型的处理系统中引入到分布式文件系统中，然后编写 MapReduce 任务以处理这些数据，将其转换回关系形式

#### 2.5.2 处理模型更为多样
基于 Hadoop 实现的各种处理模型可以**共享集群并行运行**，且不同的处理模型都可以访问 HDFS 上的相同文件。在 Hadoop 生态中，无需将数据在不同的特化系统间倒来倒去以进行不同类型的处理：**Hadoop 系统足够开放，能够以单一集群支持多种负载类型**。**无需移动数据**让我们更容易的从数据中挖掘价值，也更容易开发新的处理模型。

#### 2.5.3 面向频繁出错设计

MapReduce 在遇到某个 map 或 reduce 子任务运行出错时，可以单独、自动地进行重试，而不会引起整个 MapReduce 任务的重试。此外，MapReduce 倾向于将数据（甚至是 map 到 reduce 中间环节的数据）进行落盘，一方面是为了容错，另一方面是因为 MapReduce 在设计时假设面对的数据量足够大，内存通常装不下。

因此，MapReduce 通常更适合**大任务**：即那些需要处理大量数据、运行较长时间的任务。而巨量的数据、过长的耗时，都会使得处理过程中遇到故障司空见惯。在这种情况下，由于一个**子任务（task）** 的故障而重试整个**任务（job）** 就非常得不偿失。当然，即使只在子任务粒度进行重试，也会让那些并不出错的任务运行的更慢（数据要持久化）。但对于频繁出错的任务场景来说，这个取舍是合理的。

### 2.6 MapReduce之外

#### 2.6.1 中间状态的物化
在大多数情况下，我们事先就明确地知道某个任务的输出**只会为**同一团队的另一个任务所使用。在这种情况下，保存到分布式文件系统上的两个任务间的数据其实只是一种**中间状态**（intermediate state）：只是一种将数据从前序任务传递到后继任务的方式。在诸如推荐系统等复杂的**数据流**中，通常会包含 50~100 个 MapReduce 任务，其中绝大部分任务间的数据都属于数据流中间状态。

将中间状态写入文件的过程称为**物化**（materialization）。


MapReduce 将工作流中间结果进行物化的方式有很多缺点：
- **无谓等待**。一个 MapReduce 任务只能在所有前置依赖任务完成后才能启动。然而由 Unix 管道缀连起来的命令却能够并行运行，只要一个任务开始产生输出，下一个任务就可以开始消费处理。由于机器配置和负载的不同，总会在某些机器上出现一些执行时间过长**拖后腿的任务**（struggler）。而 MapReduce 的这种等待机制，会让单个任务拖垮整个工作流。
- **Mapper 冗余**。Mapper 职责非常简单，仅是读出前置 Reducer 产生的数据，并为之后 Reducer 的分片和排序做准备。在很多情况下，mapper 的职责其实可以并到前序任务的 Reducer 中：如果可以将 Reducer 的输出按照后继 Reducer 的要求准备好，则可将 Reducer 直接串起来，从而省去中间夹杂的 Mapper 阶段。
- **数据冗余**。在分布式文件系统中存储中间结果，意味着将数据在不同机器上冗余了几份。对于并不需要共享的中间结果来说，这种方式太过奢侈。

#### 2.6.2 数据流引擎
针对分布式系统中的批处理负载，人们开发了很多新的执行引擎。将**整个数据流看做一个任务，而非将其拆分成几个相对独立的子任务**。和 MapReduce 一样，这些引擎也会对每个数据记录在单个线程中，重复调用用户的定制函数（包裹用户逻辑）。并且会将输入数据集进行**切片**（partition），并行地执行（数据并行），然后将一个函数的输出通过网络传递给下一个函数作为输入。

和 MapReduce 不同的是，这些函数可以进行更灵活地组织，而不需要严格遵循 map 或者 reduce 格式。我们成这些函数为**算子**（operators），且 dataflow 引擎会提供多种选择，以将一个算子的数据输出导入到下一个算子（**类似数据流接线方式**）：

- **repartition + sort（sort merge join）**：一种方法是进行 repartition 并按 key 对 record 进行排序，就像 MapReduce 的 shuffle 阶段一样。该功能能够提供像 MapReduce 一样的 **sort-merge join** 和分区方式。
- **only repartition（partition hash join）**：另一种可能是接受多个输入，并且用同样的方式进行**分区**（partitioning），但是会跳过排序阶段。这对于分区哈希 join 很有用，因为该算子只关心记录的分区，但其顺序并不重要，因为总会过哈希表重新组织。
- **broadcast（broadcast hash join）**：对于广播哈希 join，一个算子的输出会被发送到多个待 join 分区算子。


- **按需 shuffle**：对于排序等高代价负载，只有在需要的时候才会执行，而不是总强制发生在 map 和 reduce 之间。
- **省掉无用 Mapper**：由于 map 本身并没有进行 repartition，因此可以将其合并到前一个算子中的 reduceer 阶段。
- **数据传输优化**：由于所有 join 和依赖等数据拓扑是显式声明的，调度器可以事先知道哪些数据在哪里被需要。因此可以尽可能地做**局部性优化**（locality optimization）。例如，可以尽量将消费某分区数据的任务放到生产该数据的机器上执行，从而通过共享内存而非网络来共享数据。
- **中间结果只存一份**：通常来说，只需要将算子的中间结果，在内存中或者本地硬盘中放一份就够了，而不用写到分布式文件系统中。在 MapReduce 中 Mapper 的输出其实也是用了此优化，只不过 dataflow 引擎将该思想扩展到了所有中间状态的存储中。
- **算子执行流水化**：大部分算子只要有输入了就可以执行，而不用等到前置任务都完成了才能够执行。
- **进程复用**：同一个工作流中，前面算子所使用的 JVM 进程池可以为之后算子所复用，而不用像 MapReduce 一样每个任务都要开一个新的 JVM 进程。

**算子**是 map 和 reduce 的泛化


#### 2.6.3 容错
将所有中间状态持久化到分布式文件系统中的一个好处是——**持久性**（durable），这会使得 MapReduce 的容错方式变得非常简单：如果某个任务挂了，仅需要在其他机器上重新启动，并从文件系统中读取相同的输入即可。

Spark、Flink 和 Tez 都会避免将中间状态写到 HDFS 中，因此他们采用了完全不同的容错方式：**如果某个机器上的中间结果丢了，就回溯工作流的算子依赖（DAG 依赖），找到最近可用的数据按照工作流重新计算**（最差的情况会一直找到输入数据，而输入数据通常存在于 HDFS 上）。

为了能够通过重新计算来容错，框架必须跟踪每一部分数据的**计算轨迹**（DGA 依赖，或者说数据谱系，data lineage）——涉及哪些输入分片、应用了哪些算子。Spark 使用**弹性分区数据集**（RDD）抽象来追踪数据的祖先；Flink 使用了快照来记录所有算子状态，以从最近的**检查点**（checkpoint）重启运行出错的算子。

当通过重算来容错时，最重要的是要明确**计算过程**（即算子）是否为**确定性的**（deterministic）：即，**给定同样的输入数据，多次运行同一算子总会产生同样的输出吗**？当算子的数据已经发到下游后出错时，该问题变的非常重要。如果算子重新运行时产生的数据和之前不一致，则下游算子很难在新老数据间进行冲突处理。对于非确定性算子的容错方案，通常是将下游算子也都清空状态一并重启。



#### 2.6.4 分布式处理框架最主要解决的两个问题是：
- **分片** 在 MapReduce 中，会根据输入数据的文件块（file chunk）的数量来调度 mappers。mappers 的输出会在**二次分片、排序、合并**（我们通常称之为 shuffle）到用户指定数量的 Reducer 中。该过程是为了将所有相关的数据（如具有相同 key）集结到一块。 后 MapReduce 时代的数据流工具会尽量避免不必要的排序（因为代价太高了），但他们仍然使用了和 MapReduce 类似的分区方式。
- **容错** MapReduce 通过频繁的（每次 MapReduce 后）**刷盘**，从而可以避免重启整个任务，而只重新运行相关子任务就可以从其故障中快速恢复过来。但在错误频率很低的情况下，这种频繁刷盘做法代价很高。数据流工具通过尽可能的减少中间状态的刷盘（当然，shuffle 之后还是要刷的），并将其尽可能的保存在内存中，但这意味着一旦出现故障就要从头重算。算子的**确定性**可以减少重算的数据范围（确定性能保证只需要算失败分区，并且结果和其他分区仍然一致）。

批处理任务的基本特点是——读取输入，进行处理，产生输出的过程中，**不会修改原数据**。换句话说，输出是输入的衍生数据。其中一个重要特点是，输入数据是**有界的**（bounded）：**输入的大小是固定的、事先确定的**（比如输入是包含一组日志的数据或者一个快照点的数据）。




