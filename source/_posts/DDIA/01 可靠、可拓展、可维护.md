---
title: "[DDIA] 可靠、可拓展、可维护"
date: 2023-05-17
tags:
  - DDIA
categories:
  - 阅读笔记
  - DDIA
published: true
toc: "true"
comments: "true"
description:
---
> 如何评价一个好数据系统，如何构建一个好的数据系统，有哪些可以遵循的设计模式？有哪些通常需要考虑的方面？

<!--more-->

## Why Data System
### 常见的数据系统
- 存储数据，以便之后再次使用——**数据库**
- 记住一些非常“重”的操作结果，方便之后加快读取速度——**缓存**
- 允许用户以各种关键字搜索、以各种条件过滤数据——**搜索引擎**
- 源源不断的产生数据、并发送给其他进程进行处理——**流式处理**
- 定期处理累积的大量数据——**批处理**
- 进行消息的传送与分发——**消息队列**

### 数据系统的复杂化
>如何评价一个好数据系统，如何构建一个好的数据系统，有哪些可以遵循的设计模式？有哪些通常需要考虑的方面？
1. **Kafka**：可以作为存储持久化一段时间日志数据、可以作为消息队列对数据进行分发、可以作为流式处理组件对数据反复蒸馏等等。
2. **Spark**：可以对数据进行批处理、也可以化小批为流，对数据进行流式处理。
3. **Redis**：可以作为缓存加速对数据库的访问、也可以作为事件中心对消息的发布订阅。
**常见的问题：**
1. 使用何种缓存策略？是旁路还是写穿透？
2. 部分组件机器出现问题时，是保证可用性还是保证一致性？
3. 当机器一时难以恢复，如何保证数据的正确性和完整性？
4. 当负载增加时，是增加机器还是提升单机性能？
5. 设计对外的 API 时，是力求简洁还是追求强大？

## 可靠性
如何衡量？
- **功能上**
    1. 正常情况下，应用行为满足 API 给出的行为
    2. 在用户误输入/误操作时，能够正常处理
- **性能上** 在给定硬件和数据量下，能够满足承诺的性能指标。
- **安全上** 能够阻止未授权、恶意破坏。
- 可用性也是可靠性的一个侧面，云服务通常以多少个 9 来衡量可用性。
两个易混淆的概念：**Fault（系统出现问题）** 和 **Failure（系统不能提供服务）**

### 硬件故障
网络抖动、不通， 硬盘损坏， 机房断电等情况


数据系统中常见的需要考虑的硬件指标：
- **MTTF mean time to failure** 单块盘 平均故障时间 5 ~10 年，如果你有 1w+ 硬盘，则均匀期望下，每天都有坏盘出现。当然事实是硬盘会一波一波坏。
解决办法，增加冗余度：机房多路供电，双网络等等。
对于数据：
- **单机**：可以做 RAID 冗余。如：EC 编码。
- **多机**：多副本 或 EC 编码。


### 软件错误
1. 不能处理特定输入，导致系统崩溃。
2. 失控进程（如循环未释放资源）耗尽 CPU、内存、网络资源。
3. 系统依赖组件变慢甚至无响应。
4. 级联故障。

在设计软件时，我们通常有一些**环境假设**，和一些**隐性约束**。随着时间的推移、系统的持续运行，如果这些假设不能够继续被满足；如果这些约束被后面维护者增加功能时所破坏；都有可能让一开始正常运行的系统，突然崩溃。


### 人为问题
- **设计编码**
    1. 尽可能消除所有不必要的假设，提供合理的抽象，仔细设计 API
    2. 进程间进行隔离，对尤其容易出错的模块使用沙箱机制
    3. 对服务依赖进行熔断设计
- **测试阶段**
    1. 尽可能引入第三方成员测试，尽量将测试平台自动化
    2. 单元测试、集成测试、e2e 测试、混沌测试
- **运行阶段**
    1. 详细的仪表盘
    2. 持续自检
    3. 报警机制
    4. 问题预案
- **针对组织**
    1. 科学的培训和管理

## 可扩展性

### 衡量负载
应对负载之前，要先找到合适的方法来衡量负载，如**负载参数（load parameters）**：
- 应用日活月活
- 每秒向 Web 服务器发出的请求
- 数据库中的读写比率
- 聊天室中同时活跃的用户数量

以 2012 年 11 月 推特为例
- 主营业务：发布推文、首页 Feed 流
- 请求量级：发布推文（平均 4.6k 请求/秒，峰值超过 12k 请求/秒），查看其他人推文（300k 请求/秒）
-  需要根据用户之间的关注与被关注关系来对数据进行多次处理。常见的有推拉两种方式：
	1. **拉**。每个人查看其首页 Feed 流时，从数据库现**拉取**所有关注用户推文，合并后呈现。
	2. **推**。为每个用户保存一个 Feed 流视图，当用户发推文时，将其插入所有关注者 Feed 流视图中。

### 描述性能
注意和系统负载区分，系统负载是从用户视角来审视系统，是一种**客观指标**。而系统性能则是描述的系统的一种**实际能力**。比如：
1. **吞吐量（throughput）**：每秒可以处理的单位数据量，通常记为 QPS。
2. **响应时间（response time）**：从用户侧观察到的发出请求到收到回复的时间。
3. **延迟（latency）**：日常中，延迟经常和响应时间混用指代响应时间；但严格来说，延迟只是指请求过程中排队等休眠时间，虽然其在响应时间中一般占大头；但只有我们把请求真正处理耗时认为是瞬时，延迟才能等同于响应时间。
响应时间通常以百分位点来衡量，比如 p95，p99 和 p999，它们意味着 95％，99％或 99.9％ 的请求都能在该阈值内完成。在实际中，通常使用滑动窗口滚动计算最近一段时间的响应时间分布，并通常以折线图或者柱状图进行呈现。

### 应对负载

如何应对负载的不断增长，即使系统具有可扩展性。
1. **纵向扩展（scaling up）或 垂直扩展（vertical scaling）**：换具有更强大性能的机器。e.g. 大型机机器学习训练。
2. **横向扩展（scaling out）或 水平扩展（horizontal scaling）**：“并联”很多廉价机，分摊负载。e.g. 马斯克造火箭。
负载扩展的两种方式：
- **自动** 如果负载不好预测且多变，则自动较好。坏处在于不易跟踪负载，容易抖动，造成资源浪费。
- **手动** 如果负载容易预测且不长变化，最好手动。设计简单，且不容易出错。

两种服务类型：
- **无状态服务** 比较简单，多台机器，外层罩一个 gateway 就行。
- **有状态服务** 根据需求场景，如读写负载、存储量级、数据复杂度、响应时间、访问模式，来进行取舍，设计合乎需求的架构。

## 可维护性
1. 友好的文档和一致的运维规范。
2. 细致的监控仪表盘、自检和报警。
3. 通用的缺省配置。
4. 出问题时的自愈机制，无法自愈时允许管理员手动介入。
5. 将维护过程尽可能的自动化。
6. 避免单点依赖，无论是机器还是人。

### 简洁性
复杂度表现：
1. 状态空间的膨胀。
2. 组件间的强耦合。
3. 不一致的术语和[命名](https://www.qtmuniao.com/2021/12/12/how-to-write-code-scrutinize-names/)。
4. 为了提升性能的 hack。
5. 随处可见的补丁（workaround）。

### 可演化性
需求一定是不断在变，引起变化的原因多种多样：
1. 对问题阈了解更全面
2. 出现了之前未考虑到的用例
3. 商业策略的改变
4. 客户爸爸要求新功能
5. 依赖平台的更迭
6. 合规性要求
7. 体量的改变

合理抽象，合理封装，对修改关闭，对扩展开放。