---
title: "[DDIA] 存储和查询"
date: 2023-05-24
tags:
  - DDIA
categories:
  - 阅读笔记
  - DDIA
published: true
toc: "true"
comments: "true"
description:
---
>数据库底层如何处理查询和存储。

<!--more-->

数据库底层如何处理查询和存储。这其中，有个**逻辑链条**：
> 使用场景 → 查询类型 → 存储格式。

**查询类型**
- **OLTP**
	- 侧重在线交易，数据量不大，瓶颈为寻找数据
	- 行存储
	- 流派
		- log-structured： 只追加，把随机写改成顺序，类似LevelDB等
		- update-in-place：以页为单位修改，主流的关系型数据库，采用b族树的
- **OLAP**
	- 侧重离线分析，数据量大
	- 列存储逐渐流行

## 1 底层数据结构
>为了加快读，我们需要构建**索引**：一种允许基于某些字段查找的额外数据结构。

存储引擎设计和选择时最常见的**权衡（trade off）**：
1. 恰当的**存储格式**能加快写（日志结构），但是会让读取很慢；也可以加快读（查找树、B 族树），但会让写入较慢。
2. 为了弥补读性能，可以构建索引。但是会牺牲写入性能和耗费额外空间。

### 1.1 哈希索引
所有数据顺序追加到磁盘上。为了加快查询，我们在内存中构建一个哈希索引：
1. Key 是查询 Key
2. Value 是 KV 条目的起始位置和长度。
3. **以顺序写代替随机写**。对于磁盘和 SSD，顺序写都要比随机写快几个数量级。
4. **简易的并发控制**。由于大部分的文件都是**不可变（immutable）** 的，因此更容易做并发读取和紧缩。也不用担心原地更新会造成新老数据交替。
5. **更少的内部碎片**。每次紧缩会将垃圾完全挤出。但是原地更新就会在 page 中留下一些不可用空间。

### 1.2 SSTables 和 LSM-Trees
>加一个限制，让这些文件按 key 有序。我们称这种格式为：SSTable（Sorted String Table）

#### 1.2.1 构建和维护
**构建 SSTable 文件**。将乱序数据在外存（磁盘 or SSD）中上整理为有序文件，是比较难的。但是在内存就方便的多。于是一个大胆的想法就形成了：
1. 在内存中维护一个有序结构（称为 **MemTable**）。红黑树、AVL 树、跳表。
2. 到达一定阈值之后全量 dump 到外存。
**维护 SSTable 文件**。为什么需要维护呢？首先要问，对于上述复合结构，我们怎么进行查询：
1. 先去 MemTable 中查找，如果命中则返回。
2. 再去 SSTable 按时间顺序由新到旧逐一查找。
如果 SSTable 文件越来越多，则查找代价会越来越大。因此需要将多个 SSTable 文件合并，以减少文件数量，同时进行 GC，我们称之为**紧缩**（ Compaction）。


#### 1.2.2 从 SSTables 到 LSM-Tree
- 将前面几节的一些碎片有机的组织起来，便是时下流行的存储引擎 LevelDB 和 RocksDB 后面的存储结构：LSM-Tree
- LSM-Tree 的核心思想——**保存一组合理组织、后台合并的 SSTables** ——简约而强大。可以方便的进行范围遍历，可以变大量随机为少量顺序。
#### 1.2.3 性能优化
**优化 SSTable 的查找**。常用 [**Bloom Filter**](https://www.qtmuniao.com/2020/11/18/leveldb-data-structures-bloom-filter/)。该数据结构可以使用较少的内存为每个 SSTable 做一些指纹，起到一些初筛的作用。
**层级化组织 SSTable**。以控制 Compaction 的顺序和时间。常见的有 size-tiered 和 leveled compaction。LevelDB 便是支持后者而得名。前者比较简单粗暴，后者性能更好，也因此更为常见。


### 1.3 B 族树
1. 以页（在磁盘上叫 page，在内存中叫 block，通常为 4k）为单位进行组织。
2. 页之间以页 ID 来进行逻辑引用，从而组织成一颗磁盘上的树。

- **查找**。从根节点出发，进行二分查找，然后加载新的页到内存中，继续二分，直到命中或者到叶子节点。查找复杂度，树的高度—— O(lgn)，影响树高度的因素：分支因子（分叉数，通常是几百个）。
- **插入 or 更新**。和查找过程一样，定位到原 Key 所在页，插入或者更新后，将页完整写回。如果页剩余空间不够，则分裂后写入。

#### 1.3.1 可靠保证：
在树结构调整时，可能会级联修改很多 Page。比如叶子节点分裂后，就需要写入两个新的叶子节点，和一个父节点（更新叶子指针）。
1. 增加预写日志（WAL），将所有修改操作记录下来，预防宕机时中断树结构调整而产生的混乱现场。
2. 使用 latch 对树结构进行并发控制。

#### 1.3.2 优化
1. 不使用 WAL，而在写入时利用 Copy On Write 技术。同时，也方便了并发控制。如 LMDB、BoltDB。
2. 对中间节点的 Key 做压缩，保留足够的路由信息即可。以此，可以节省空间，增大分支因子。
3. 为了优化范围查询，有的 B 族树将叶子节点存储时物理连续。但当数据不断插入时，维护此有序性的代价非常大。
4. 为叶子节点增加兄弟指针，以避免顺序遍历时的回溯。即 B+ 树的做法，但远不局限于 B+ 树。
5. B 树的变种，分形树，从 LSM-tree 借鉴了一些思想以优化 seek。

#### 1.3.3 对比
- LSM-Tree  写入更快，更加紧凑  但是同一个 Key 存多遍
- B Tree 读取更快，范围锁更方便

### 1.4 其他索引结构
**次级索引（secondary indexes）**。即，非主键的其他属性到该元素（SQL 中的行，MongoDB 中的文档和图数据库中的点和边）的映射。

#### 1.4.1 聚集索引和非聚集索引（cluster indexes and non-cluster indexes）
1. 数据本身**无序**的存在文件中，称为 **堆文件（heap file）**，索引的值指向对应数据在 heap file 中的位置。这样可以避免多个索引时的数据拷贝。
2. 数据本身按某个字段有序存储，该字段通常是主键。则称基于此字段的索引为**聚集索引**（clustered index），从另外一个角度理解，即将索引和数据存在一块。则基于其他字段的索引为**非聚集索引**，在索引中仅存数据的引用。
3. 一部分列内嵌到索引中存储，一部分列数据额外存储。称为**覆盖索引（covering index）**   或  **包含列的索引（index with included columns）**。

#### 1.4.2 多列索引
1. 将二维编码为一维，然后按普通索引存储。
2. 使用特殊数据结构，如 R 树。

#### 1.4.3 全文索引和模糊索引
前述索引只提供全字段的精确匹配，而不提供类似搜索引擎的功能。比如，按字符串中包含的单词查询，针对笔误的单词查询。

在工程中常用 [Apace Lucene](https://lucene.apache.org/ "Apace Lucene") 库，和其包装出来的服务：[Elasticsearch](https://www.elastic.co/cn/ "Elasticsearch")。他也使用类似 LSM-tree 的日志存储结构，但使用其索引进行模糊匹配的过程，本质上是一个有限状态自动机，在行为上类似 Trie 树。


#### 1.4.4 全内存数据结构
根据是否需要持久化，内存数据大概可以分为两类：
1. **不需要持久化**。如只用于缓存的 Memcached。
2. **需要持久化**。通过 WAL、定期 snapshot、远程备份等等来对数据进行持久化。但使用内存处理全部读写，因此仍是内存数据库。

## 2 事务型还是分析型
术语 **OL**（Online）主要是指交互式的查询。
术语**事务**（transaction）由来有一些历史原因。早期的数据库使用方多为商业交易（commercial），比如买卖、发工资等等。但是随着数据库应用不断扩大，交易\事务作为名词保留了下来。

事务不一定具有 ACID 特性，事务型处理多是随机的以较低的延迟进行读写，与之相反，分析型处理多为定期的批处理，延迟较高。

#### 2.1.1 数据仓库
需要一种手段将数据从原库导入到专门的**数仓**。
我们称之为 **ETL：extract-transform-load**。
### 2.2 AP 建模：星状型和雪花型
星状模型通常包含一张**事件表（_fact table_）** 和多张**维度表（_dimension tables_）**。事件表以事件流的方式将数据组织起来，然后通过外键指向不同的维度。

星状模型的一个变种是雪花模型，可以类比雪花（❄️）图案，其特点是在维度表中会进一步进行二次细分，讲一个维度分解为几个子维度。比如品牌和产品类别可能有单独的表格。星状模型更简单，雪花模型更精细，具体应用中会做不同取舍。

在典型的数仓中，事件表可能会非常宽，即有很多的列：一百到数百列。


## 3 列存

**维度表**和**事实表**，对于后者来说，有可能达到数十亿行和数 PB 大。虽然事实表可能通常有几十上百列，但是单次查询通常只关注其中几个维度（列）。

### 3.1 列压缩
将所有数据分列存储在一块，带来了一个意外的好处，由于同一属性的数据相似度高，因此更易压缩。

如果每一列中值阈相比行数要小的多，可以用**位图编码（_[bitmap encoding](https://en.wikipedia.org/wiki/Bitmap_index "bitmap encoding")_）**。举个例子，零售商可能有数十亿的销售交易，但只有 100,000 个不同的产品。

### 3.2 列式存储的排序
可以如 LSM-Tree 一样，对所有行按某一列进行排序后存储。
- 不同副本，不同排序

### 3.3 列式存储的写入
1. 将新写入的数据在**内存**中 Batch 好，按行按列，选什么数据结构可以看需求。
2. 然后达到一定阈值后，批量刷到**外存**，并与老数据合并。

### 3.4 聚合：数据立方和物化视图
数据仓库查询通常涉及聚合函数，如 SQL 中的 COUNT、SUM、AVG、MIN 或 MAX。如果这些函数被多次用到，每次都即时计算显然存在巨大浪费。因此一个想法就是，能不能将其缓存起来。

其与关系数据库中的**视图**（View）区别在于，视图是**虚拟的、逻辑存在**的，只是对用户提供的一种抽象，是一个查询的中间结果，并没有进行持久化（有没有缓存就不知道了）。

物化视图本质上是对数据的一个摘要存储，如果原数据发生了变动，该视图要被重新生成。因此，如果**写多读少**，则维持物化视图的代价很大。但在数仓中往往反过来，因此物化视图才能较好的起作用。

物化视图一个特化的例子，是**数据立方**（data cube，或者 OLAP cube）：按不同维度对量化数据进行聚合。