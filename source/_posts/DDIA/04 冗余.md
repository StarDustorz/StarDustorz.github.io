---
title: "[DDIA] 冗余"
date: 2023-06-14
tags:
  - DDIA
categories:
  - 阅读笔记
  - DDIA
published: true
toc: "true"
comments: "true"
description:
---
>**冗余（Replication）** 是指将同一份数据复制多份，放到通过网络互联的多个机器上去。

<!--more-->

其好处有：
1. **降低延迟**：可以在地理上同时接近不同地区的用户。
2. **提高可用性**：当系统部分故障时仍然能够正常提供服务。
3. **提高读吞吐**：平滑扩展可用于查询的机器。

- 如果只读，直接复制即可。可以利用这个特性，使用分治策略，将数据分为只读部分和读写部分，则只读部分的冗余就会容易处理的多
- 允许数据变更时，如何维护多机冗余且一致。常用的冗余控制算法有：
	- 单领导者（single leader）
	- 多领导者（multi-leader）
	- 无领导者（leaderless）
- 需要在多方面做取舍
	- 使用同步复制还是异步复制
	- 如何处理失败的副本

## 1 领导与跟随者
**副本**：冗余的每份数据。  如何保证所有的数据都同步到了所有的副本上？
基于**领导者（leader-based）** 的同步算法：也就是主从同步
1. 其中一个副本称为**领导者**（leader），别称**主副本**（primary、master）。主副本作为写入的协调者，所有写入都要发给主副本。
2. 其他副本称为**跟随者**（follower），也称为**只读副本**（read replicas）、**从副本**（slaves）、**次副本**（secondaries）、**热备**（hot-standby）。主副本将改动写到本地后，将其发送给各个从副本，从副本收变动到后应用到自己状态机，这个过程称为**日志同步**（replication log）、**变更流**（change steam）。
3. 对于读取，客户端可以从主副本和从副本中读取；但写入，客户端只能将请求发到主副本。
### 1.1 同步和异步复制
**同步（synchronously）复制**和**异步（asynchronously）复制**和关键区别在于：请求何时返回给客户端。
1. 如果等待某副本写完成后，则该副本为同步复制。
2. 如果不等待某副本写完成，则该副本为异步复制。

对比：
1. 同步复制牺牲了**响应延迟**和**部分可用性**（在某些副本有问题时不能完成写入操作），换取了所有副本的一致性（但并不能严格保证）。
2. 异步复制放松了**一致性**，而换来了较低的写入延迟和较高的可用性。

取舍：
1. **全同步**：所有的从副本都同步写入。如果副本数过多，可能性能较差，当然也可以做并行化、流水线化处理。
2. **半同步**：（**semi-synchronous**），有一些副本为同步，另一些副本为异步。
3. **全异步**：所有的从副本都异步写入。网络环境比较好的话，可以这么配置。

### 1.2 新增副本
如果原副本是只读（read-only）的，只需要简单拷贝即可。但是如果是可写副本，则问题要复杂很多。因此，比较简单的一种解决方法是：禁止写入，然后拷贝。这在某些情况下很有用，比如夜间没有写入流量，同时一晚上肯定能复制完。
如果要不停机，可以：
1. 主副本在本地做**一致性**快照。何谓一致性？
2. 将快照复制到从副本节点。
3. 从主副本拉取快照之后的操作日志，应用到从副本。如何知道快照与其后日志的对应关系？序列号。
4. 当从副本赶上主副本进度后，就可以正常跟随主副本了。

### 1.3 宕机处理
系统中任何节点都可能在计划内或者计划外宕机。那么如何应对这些宕机情况，保持整个系统的可用性呢？
#### 1.3.1 主副本宕机：故障转移
首先要选出新的主副本，然后要通知所有客户端主副本变更：
1. **确认主副本故障**。要防止由于网络抖动造成的误判。一般会用心跳探活，并设置合理超时（timeout）阈值，超过阈值后没有收到该节点心跳，则认为该节点故障。
2. **选择新的主副本**。新的主副本可以通过**选举**（共识问题）或者**指定**（外部控制程序）来产生。选主时，要保证备选节点数据尽可能的新，以最小化数据损失。
3. **让系统感知新主副本**。系统其他参与方，包括从副本、客户端和旧主副本。前两者不多说，旧主副本在恢复时，需要通过某种手段，让其知道已经失去领导权，避免**脑裂**。

**主副切换问题**：
1. **新老主副本数据冲突**。新主副本在上位前没有同步完所有日志，旧主副本恢复后，可能会发现和新主副本数据冲突。
2. **相关外部系统冲突**。即新主副本，和使用该副本数据的外部系统冲突。书中举了 github 数据库 MySQL 和缓存系统 redis 冲突的例子。
3. **新老主副本角色冲突**。即新老主副本都以为自己才是主副本，称为**脑裂（split brain）**。如果他们两个都能接受写入，且没有冲突解决机制，数据会丢失或者损坏。有的系统会在检测到脑裂后，关闭其中一个副本，但设计的不好可能将两个主副本都关闭。
4. **超时阈值选取**。如果超时阈值选取的过小，在不稳定的网络环境中（或者主副本负载过高）可能会造成主副本频繁的切换；如果选取过大，则不能及时进行故障切换，且恢复时间也增长，从而造成服务长时间不可用。

节点故障；不可靠网络；在一致性、持久化、可用性和延迟间的取舍；等等问题，都是设计分布式系统时，所面临的的基本问题。根据实际情况，对这些问题进行艺术化的取舍，便是分布式系统之美。

#### 1.3.2 从副本宕机：追赶恢复。
类似于新增从副本。如果落后的多，可以直接向主副本拉取快照 + 日志；如果落后的少，可以仅拉取缺失日志。


### 1.4 日志复制
在数据库中，基于领导者的多副本是如何实现的？在不同层次有多种方法，包括：
1. **语句层面的复制。**
2. **预写日志的复制**。
3. **逻辑日志的复制**。
4. **触发器的复制**。

对于一个**系统**来说，多副本同步的是什么？**增量修改**。
具体到一个由数据库构成的**数据系统**，通常由数据库外部的**应用层**、数据库内部**查询层**和**存储层**组成。**修改**在查询层表现为：语句；在存储层表现为：存储引擎相关的预写日志、存储引擎无关的逻辑日志；修改完成后，在应用层表现为：触发器逻辑。

#### 1.4.1 基于语句
主副本记录下所有更新语句：`INSERT`、`UPDATE`  或  `DELETE` 然后发给从库。主副本在这里类似于充当其他从副本的**伪客户端**。

#### 1.4.2 传输预写日志（WAL）
主流的存储引擎都有**预写日志**（WAL，为了宕机恢复）：
1. 对于日志流派（LSM-Tree，如 LevelDB），每次修改先写入 log 文件，防止写入 MemTable 中的数据丢失。
2. 对于原地更新流派（B+ Tree），每次修改先写入 WAL，以进行崩溃恢复。
所有用户层面的改动，最终都要作为状态落到存储引擎里，而存储引擎通常会维护一个：
1. 追加写入
2. 可重放
这种结构，天然适合备份同步。本质是因为磁盘的读写特点和网络类似：**磁盘是顺序写比较高效，网络是只支持流式写**。具体来说，主副本在写入 WAL 时，会同时通过网络发送对应的日志给所有从副本。

#### 1.4.3 逻辑日志
和具体的存储引擎物理格式解耦，在做数据同步时，可以使用不同的日志格式：**逻辑日志**。
对于关系型数据库来说，行是一个合适的粒度：
1. **对于插入行**：日志需包含所有列值。
2. **对于删除行**：日志需要包含待删除行标识，可以是主键，也可以是其他任何可以唯一标识行的信息。
3. **对于更新行**：日志需要包含待更新行的标志，以及所有列值（至少是要更新的列值）
对于多行修改来说，比如事务，可以在修改之后增加一条事务提交的记录。MySQL 的 binlog 就是这么干的。
使用逻辑日志的**好处**有：
1. 方便新旧版本的代码兼容，更好的进行滚动升级。
2. 允许不同副本使用不同的存储引擎。
3. 允许导出变动做各种**变换**。如导出到数据仓库进行离线分析、建立索引、增加缓存等等。

## 2 复制滞后问题
使用多副本的好处有：
1. **可用性**：容忍部分节点故障
2. **可伸缩性**：增加读副本处理更多读请求
3. **低延迟**：让用户选择一个就近的副本访问
对于读多写少的场景，想象中，可以通过使劲增加读副本来均摊流量。但有个**隐含**的条件是，多副本间的同步得做成**异步**的，否则，读副本一多，某些副本就很容易出故障，进而阻塞写入。
### 2.1 读你所写
在一个**异步复制**的分布式数据库里，同一个客户端，写入**主副本**后返回；稍后再去读一个落后的**从副本**，就会发现：读不到自己刚写的内容！
为了避免这种反直觉的事情发生，我们引入一种新的一致性：**读写一致性（read-after-write consistency）**，或者  **读你所写一致性（read-your-writes consistency）**。
**列举几种方案：**
1. **按内容分类**。对于客户端可能修改的内容集，**只从主副本读取**。如社交网络上的个人资料，读自己的资料时，从主副本读取；但读其他人资料时，可以向从副本读。
2. **按时间分类**。如果每个客户端都能访问基本所有数据，则方案一就会退化成所有数据都要从主副本读取，这显然不可接受。此时，可以按时间分情况讨论，近期内有过改动的数据，从主副本读，其他的，向从副本读。那这个区分是否最近的**时间阈值**（比如一分钟）如何选取呢？可以监控从副本一段时间内的最大延迟这个经验值，来设置。
3. **利用时间戳**。客户端记下本客户端上次改动时的时间戳，在读从副本时，利用此时间戳来看某个从副本是否已经同步了改时间戳之前内容。可以在所有副本中找到一个已同步了的；或者阻塞等待某个副本同步到改时间戳后再读取。时间戳可以是逻辑时间戳，也可以是物理时间戳（此时多机时钟同步非常重要）。

### 2.2 单调读
对于一个客户端来说，系统可能会发生**时光倒流（moving backward in time）**。
再引入一种一致性保证：**单调读（Monotonic reads）**。
- 读写一致性和单调读有什么区别？ 写后读保证的是写后读顺序，单调读保证的是**多次读**之间的顺序。
如何实现单调读？
1. 只从一个副本读数据。
2. 前面提到的时间戳机制。
### 2.3 一致前缀读
异步复制所带来的第三个问题：有时候会违反因果关系。
本质在于：如果数据库由多个分区（Partition）组成，而分区间的事件顺序无法保证。此时，如果有因果关系的两个事件落在了不同分区，则有可能会出现**果在前，因在后**。
为了防止这种问题，我们又引入了一种一致性：**一致前缀读（consistent prefix reads）**。奇怪的名字。
实现这种一致性保证的方法：
1. 不分区。
2. 让所有有因果关系的事件路由到一个分区

### 2.4 终极解决方案
**事务！**
多副本异步复制所带来的一致性问题，都可以通过**事务（transaction）** 来解决。单机事务已经存在了很长时间，但在数据库走向分布式时代，一开始很多 NoSQL 系统抛弃了事务。
- 这是为什么？
    1. 更容易的实现。2. 更好的性能。3. 更好的可用性。
于是复杂度被转移到了应用层。
这是数据库系统刚大规模步入分布式（**多副本、多分区**）时代的一种妥协，在经验积累的够多之后，事务必然会被引回。
于是近年来越来越多的分布式数据库开始支持事务，是为**分布式事务**。

## 3 多主模型
**单主模型一个最大问题**：所有写入都要经过它，如果由于任何原因，客户端无法连接到主副本，就无法向数据库写入。
**多主复制（multi-leader replication）**：有多个可以接受写入的主副本，每个主副本在接收到写入之后，都要转给所有其他副本。即一个系统，有多个**写入点**。

1. 数据库横跨多个数据中心
2. 需要离线工作的客户端
3. 协同编辑
## 4 无主模型
通常来说，在无主模型中，写入时可以：
1. 由客户端直接写入副本。
2. 由**协调者（coordinator）** 接收写入，转发给多副本。但与主副本不同，协调者并不负责定序。
### 4.1 有节点故障时的写入
多数派写入，多数派读取，以及读时修复。
由于写入时，简单的忽略了宕机副本；在读取时，就要多做些事情了：**同时读取多个副本，选取最新_版本_的值**。

#### 4.1.1 读时修复和反熵
无主模型也需要维持多个副本数据的一致性。在某些节点宕机重启后，如何让其弥补错过的数据？
1. **读时修复（read repair）**，本质上是一种捎带修复，在读取时发现旧的就顺手修了。
2. **反熵过程（Anti-entropy process）**，本质上是一种兜底修复，读时修复不可能覆盖所有过期数据，因此需要一些后台进程，持续进行扫描，寻找陈旧数据，然后更新

#### 4.1.2 Quorum 读写
如果副本总数为 n，写入 w 个副本才认定写入成功，并且在查询时最少需要读取 r 个节点。只要满足 w + r > n，我们就能读到最新的数据（**鸽巢原理**）。此时 r 和 w 的值称为 **quorum 读写**。即这个约束是保证数据有效所需的最低（法定）票数。

在 Dynamo 流派的存储中，n、r 和 w 通常是可以配置的：
1. n 越大冗余度就越高，也就越可靠。
2. r 和 w 都常都选择超过半数，如 `(n+1)/2`
3. w = n 时，可以让 r = 1。此时是牺牲写入性能换来读取性能。

考量满足 w+r > n 系统对节点故障的容忍性：
1. 如果 w < n，则有节点不可用时，仍然能正常写入。
2. 如果 r < n，则有节点不可用时，仍然能正常读取。

通常来说，我们会将读或者写并行的发到全部 n 个副本，但是只要等到法定个副本的结果，就可以返回。
如果由于某种原因，可用节点数少于 r 或者 w，则读取或者写入就会出错。

### 4.2 quorum 一致性的局限

1. 使用宽松的 Quorum 时（n 台机器范围可以发生变化），w 和 r 可能并没有交集。
2. 对于写入并发，如果处理冲突不当时。比如使用 last-win 策略，根据本地时间戳挑选时，可能由于时钟偏差造成数据丢失。
3. 对于读写并发，写操作仅在部分节点成功就被读取，此时不能确定应当返回新值还是旧值。
4. 如果写入节点数 < w 导致写入失败，但并没有对数据进行回滚时，客户端读取时，仍然会读到旧的数据。
5. 虽然写入时，成功节点数 > w，但中间有故障造成了一些副本宕机，导致成功副本数 < w，则在读取时可能会出现问题。
6. 即使都正常工作，也有可能出现一些关于时序（timing）的边角情况。

#### 4.2.1 一致性监控
对于无主模型，由于没有固定写入顺序，副本的落后进度变得难以界定。如果系统只使用读时修复策略，则对于一个副本的落后程度是没有限制的。读取频率很低数据可能版本很老。

最终一致性是一种很模糊的保证，但通过监控能够量化“最终”（比如到一个阈值），也是很棒的。


### 4.3 放松的 Quorum 和提示转交
正常的 Quorum 能够容忍一些副本节点的宕机。但在大型集群（总节点数目 > n）中，可能最初选中的 n 台机器，由于种种原因（宕机、网络问题），导致无法达到法定读写数目，则此时有两种选择：
1. 对于所有无法达到 r 或 w 个法定数目的读写，直接报错。
2. 仍然接受写入，并且将新的写入暂时交给一些正常节点。
后者被认为是一种**宽松的法定数目** （**sloppy quorum**）：写和读仍然需要 w 和 r 个成功返回，但是其所在节点集合可以发生变化。


一旦问题得到解决，数据将会根据线索移回其应该在的节点（D—> B），我们称之为**提示移交**（hinted handoff）。这个移交过程是由反熵 anti-entropy 后台进程完成的。

这是一种典型的牺牲部分一致性，换取更高可用性的做法。在常见的 Dynamo 实现中，放松的法定人数是可选的。在 Riak 中，它们默认是启用的，而在 Cassandra 和 Voldemort 中它们默认是禁用的


#### 4.3.1 多数据中心
为了同时兼顾**多数据中心**和**写入的低延迟**，有一些不同的基于无主模型的多数据中心的策略：
1. 其中 Cassandra 和 Voldemort 将 n 配置到所有数据中心，但写入时只等待本数据中心副本完成就可以返回。
2. Riak 将 n 限制在一个数据中心内，因此所有客户端到存储节点的通信可以限制到单个数据中心内，而数据复制在后台异步进行。

### 4.4 并发写入检测
由于 Dynamo 允许多个客户端并发写入相同 Key，则即使使用严格的 Quorum 读写，也会产生冲突：**对于时间间隔很短（并发）的相同 key 两个写入，不同副本上收到的顺序可能不一致**。
此外，读时修复和提示移交时，也可能产生冲突。
为了使所有副本最终一致，需要有一种手段来解决并发冲突。

#### 4.4.1 后者胜
后者胜（LWW，last write wins）的策略是，通过某种手段确定一种全局唯一的顺序，然后让后面的修改覆盖之前的修改。

如，为所有写入附加一个全局时间戳，如果对于某个 key 的写入有冲突，可以挑选具有最大时间戳的数据保留，并丢弃较早时间戳的写入。
使用 LWW 唯一安全的方法是：key 是一次可写，后变为只读。

#### 4.4.2 发生于之前（Happens-before）和并发关系
1. 服务器为每个键分配一个版本号 V，每次该键有写入时，将 V + 1，并将版本号与写入的值一块保存。
2. 当客户端读取该键时，服务器将返回所有未被覆盖的值以及最新的版本号。
3. 客户端在进行下次写入时，必须**包含**之前读到的版本号 Vx（说明基于哪个版本进行新的写入），并将读取的值合并到一块。
4. 当服务器收到特定版本号 Vx 的写入时，可以用其值覆盖所有 V ≤ Vx 的值。

#### 4.4.3 版本向量
每个副本在遇到写入时，会增加对应键的版本号，同时跟踪从其他副本中看到的版本号，通过比较版本号大小，来决定哪些值要覆盖哪些值要保留。