---
title: "[DDIA] 一致性和共识协议"
date: 2023-07-22
tags:
  - DDIA
categories:
  - 阅读笔记
  - DDIA
published: true
toc: "true"
comments: "true"
description:
---
>**共识**（consensus），即，_让所有节点在**某件事情**上达成一致_。

<!--more-->

大部分**数据系统**都可以抽象为一系列**数据操作**的依次施加，即状态机模型。而共识协议可以让多机对某个**确定**的**操作序列**达成共识，进而对系统的任意状态达成共识。

## 1 一致性保证
**收敛性**（_convergence_），即最终，所有副本都会收敛到相同的值。


## 2 线性一致性
一个系统对外表现的像所有数据**只有一个副本**，作用于数据上的操作都可以**原子地完成**。
在一个提供线性一致性的系统中，只要某个客户端成功的进行了写入某值，其他所有客户端都可以在数据库中读到该值。提供单副本的抽象，意味着客户端任何时刻读到的都是**最近、最新**（up-to-date）的值，而不会是过期缓存、副本中的旧值。换句话说，线性一致性是一种数据**新鲜度保证**（recency guarantee）。

### 2.1 如何让系统满足线性一致？
**让系统表现得好像只有一个数据副本**。

线性一致性要求所有操作标记组成序列是**永远向前的**，即满足数据**新鲜度要求：一旦我们写入或者读取到某值，所有稍后的读请求都能看到该值，直到有人再次将其改写。**

**线性一致性和可串行化**

**线性一致性**（Linearizability）很容易和**可串行化**（serializability）相混淆，因为他们看起来都像是：可以进行拓扑化组织。但他们是不同维度的约束，我们很有必要对其进行区分：

**可串行化**（Serializability）。可串行化是事务的一种隔离级别。每个事务可能会涉及**多个数据对象**（行、文档、记录）的读写，[之前](https://ddia.qtmuniao.com/#/ch07?id=%e5%8d%95%e5%af%b9%e8%b1%a1%e5%92%8c%e5%a4%9a%e5%af%b9%e8%b1%a1%e6%93%8d%e4%bd%9c)有讨论过单对象和多对象。可串行化可以保证所有事务好像按**某种顺序依次执行**（后一个事务在前一个事务结束后才开始）。需要注意的是，如果某种串行顺序和实际执行顺序不一致也没事，只要是串行执行就行。举个例子，如果 A、B、C 三个事务并发执行，真实顺序是 A->B->C，但如果对应用层表现为 C->A->B 的执行顺序（可能由于多机时间戳不同步），也可以叫可串行化，但 C->A->B 的执行顺序在**某个**对象上可能不满足线性一致性。

**线性一致性**（Linearizability）。线性一致性是一种针对寄存器（register，**单个数据对象**）的读写新鲜度保证。它不会将多个操作打包成事务，因此不能避免像之前提到的[写偏序](https://ddia.qtmuniao.com/#/ch07?id=%e5%86%99%e5%81%8f%e5%ba%8f%e5%92%8c%e5%b9%bb%e8%af%bb)等问题，除非使用某些辅助手段，如[物化冲突](https://ddia.qtmuniao.com/#/ch07?id=%e7%89%a9%e5%8c%96%e5%86%b2%e7%aa%81)。

一个数据库可以同时提供可串行化和线性一致性保证，我们称之为**严格可串行化**（_strict serializability_）或者**单副本可串行化**（strong one-copy serializability）。使用两阶段锁或者真正串行化执行实现的可串行化，通常都是线性一致的。

然而，基于快照隔离的串行化**通常**不是线性一致的。为了避免读写互相阻塞，所有的读取都会基于某个一致性的快照，则该快照之后的写入不会反映到读请求上，因此，快照读不满足线性一致性。

### 2.2 应用线性一致性
在使用单主模型的系统中，需要保证任何时刻只有一个主副本，而非多个（脑裂）。一种进行主选举的方法是使用锁：每个节点在启动时都试图去获取锁，最终只有一个节点会成功并且变为主。不论使用什么方式实现锁，都必须**满足线性一致性**：所有节点必须就某节点拥有锁达成一致，否则这样的锁服务是不能用的。

**唯一性约束**在数据库中很常见：比如用户名和邮箱可以用来唯一的标识一个用户、在同一个文件系统中不可能有多个文件具有相同的路径和文件名。如果你想在数据写入时**维持这些约束**（比如两个人使用相同的用户名并发地创建账户，其中一个会失败而报错），你需要线性一致性。

要求所有节点在**单个最新值**（账户余额、股票水位、座位预定）上达成一致。


### 2.3 实现线性一致的系统
可以通过牺牲部分性能来让 Dynamo 风格的 Quorum 读写变成线性一致的：
1. 每个读请求必须进行同步的[读取修复](https://ddia.qtmuniao.com/#/ch05?id=%e8%af%bb%e6%97%b6%e4%bf%ae%e5%a4%8d%e5%92%8c%e5%8f%8d%e7%86%b5)。
2. 发送任意写请求之前要先读取最新值。

### 2.4 线性一致性的代价
如果应用层要求线性一致的读写，则**数据中心间的网络中断会造成服务的不可用**。

#### 2.4.1 CAP 定理
- 如果应用层要求系统提供线性一致性，此时如果某些数据副本由于网络问题和系统其他部分**断开了连接**，则这些数据副本就**不再能够正常地处理请求**：要么等待网络恢复、要么进行报错。但这都意味着系统不可用。
- 如果应用不要求系统的线性一致，则即使多副本间遇到连接问题，每个副本可以独立的进行写入。从而，即使出现了网络故障，系统仍然能够保持可用，但其行为不是线性一致的。

CAP 有时候被表述为，在做系统设计时，一致性（consistency）、可用性（Availability）、分区容错性（Partition tolerance），只能三取其二。
**当网络出现分区时，一致性和可用性只能二选其一**（_either Consistent or Available when Partitioned_）。

#### 2.4.2 线性一致性和网络延迟

很多分布式系统选择不提供线性一致性的原因也在于此：**是为了提升系统性能而非进行容错**。在任何时候，提供线性一致性都会严重拖慢系统。而非在网络故障发生时，才需要对线性一致性进行牺牲。

如果你想要保证线性一致，读写请求的响应时间是**正比于网络延迟**的。

**顺序性**（ordering）、**线性一致性**（linearizability）和**共识协议**（consensus）三个概念间有很深的联系。

## 3 顺序保证
### 3.1 顺序和因果（Ordering and Causality）
**顺序可以维持因果性**
因果将顺序施加于**事件**（event）：
1. 先有因，后有果
2. 先有消息发送，然后该消息被收到
3. 先有问题，后有答案
如果一个系统遵循因果约束，则我们称其为**因果一致的**（_causally consistent_）。比如，快照隔离就可以提供因果一致性：当从数据库读取数据的时候，如果你能读到某个时间点的数据，就一定能读到其之前的数据

#### 3.1.1 因果序非全序
**全序**（total order）意味着**系统内任意两个元素可比大小**。
反之，集合是**偏序**（partially ordered）：在某些情况下，我们可以说一个集合比另一个集合大（两个集合间有包含关系）；但在另外一些情况下，两个集合间没有可比关系。

**线性一致性**（Linearizability）：对于任意两个操作，我们总是可以确定其发生的先后关系，也即在可线性化系统中，所有的操作顺序满足全序关系。
**因果一致性**（Causality）。如果我们无从判定两个操作的先后关系，则称之为**并发的**（concurrent，参见[发生于之前和并发关系](https://ddia.qtmuniao.com/#/ch05?id=%e5%8f%91%e7%94%9f%e4%ba%8e%e4%b9%8b%e5%89%8d%ef%bc%88happens-before%ef%bc%89%e5%92%8c%e5%b9%b6%e5%8f%91%e5%85%b3%e7%b3%bb)）。因果性定义了一种**偏序**（partial order）关系，而非全序关系：有些操作存在因果，因此可比；而另外一些操作则是并发的，即不可比。

### 3.2 序列号定序
**使用序列号（sequence numbers）或者时间戳（timestamps）来给事件定序**。
在使用单主模型的多副本系统中，主节点上**操作日志的追加顺序**确定了一个对所有操作的全序，且满足操作发生的因果关系。主节点可以为每条日志按顺序关联一个全局递增的序列号，如果从节点上也按都按此序列号顺序应用操作日志到状态机，则每个副本总能保持一致的状态（但有可能稍落后于主节点）。

#### 3.2.1 非因果序生成器
如果系统中没有唯一的单主节点（比如你用的是多主模型或无主模型，又或者你的系统存在多个分区），则如何为每个操作产生一个序列号就变得不那么简单直观了。常用的方式有以下几种：
1. **每个节点独立地生成不相交的序列集**。如，你的系统中有两个节点，一个节点只产生奇数序号，另一个节点只产生偶数序号。更通用一些，我们可以在生成的序号中保留一些位来编码对节点的标识，从而让不同的节点永远不会产生相同的序号。
2. **可以为每个操作关联一个日历时钟**（或者说物理时钟）。这些时间戳不是有序的（因为回拨？），但如果有足够的精读，就可以让任意两个操作关联的时间戳不同，依次也可以达到全序的目的。此种方法有时候会被用在解决冲突使用后者胜的策略（但会有风险）。
3. **每次可以批量产生一组序列号**。比如，在请求序列号时，节点 A 可以一次性声明占用 1 ~ 1000 的序列号，节点 B 会一次占用 1001~2000 的序列号。则本地的操作可以从拿到的这批序列号中直接分配，仅在快耗尽时再去请求一批。这种方法常被用在 TSO（timestamp oracle，单点授时）的优化中。
**都存在因果问题：**
1. **不同节点上处理操作的速率很难完全同步**。因此，如果一个节点使用奇数序号，另一个节点时用偶数序号，则两个序号消耗的速率也会不一致。此时，当你有两个奇偶性不同的序号时，就难以通过比较大小来确定操作发生的先后顺序。
2. **物理时间戳会由于多机时钟偏差，而不满足因果一致**。例如，在图 8-3 中（参见[时间戳以定序](https://ddia.qtmuniao.com/#/ch08?id=%e6%97%b6%e9%97%b4%e6%88%b3%e4%bb%a5%e5%ae%9a%e5%ba%8f)），就出现了发生在之后的操作被分配了一个较小的时间戳。
3. 对于批量分配方式，有可能发生较早的操作被分配了 1001-2000 的序列号，而较晚的操作被分配了 1-1000 的序列号。如此一来，序列号的分配不满足因果一致。


#### 3.2.2 Lamport 时间戳
在该系统中，每个节点有一个唯一的 **id** 和一个记录处理过多少个操作的计数器，Lamport 时间戳是上述两者组成的二元组：`(counter, node ID)` 。不同的节点可能会有相同的 counter 值，但通过引入 node ID，可以使所有时间戳都是全局唯一的。

让 Lamport 时间戳能够满足因果一致性的核心点在于：**每个节点和客户端都会让 counter 追踪当前所看到（包括本机的和通信的）的最大值**。当节点看到请求或者回复中携带的 counter 值比自己大，就会立即用其值设置本地 counter。

**只有在收集到系统中所有操作之后，才能真正确定所有操作的全序**。


### 3.3 全序广播
使用单主模型的系统会面临两个问题：
1. 当系统负载超过单机可以处理的尺度，如何进行扩容。
2. 当主节点宕机时如何进行故障转移（failover）。
在分布式系统的语境下，该问题也被称为**全序广播**（total order broadcast）或者**原子广播**（atomic broadcast）。


全序广播是一种多个节点间交换消息的协议。它要求系统满足两个安全性质：
1. **可靠交付**。如果一个节点收到了消息，则系统内所有的相关节点都要收到该消息。
2. **全序交付**。每个节点接收到消息的顺序一致。

全序广播的一个重要性质是：**当收到消息时，其顺序已经确定**。

可以从另外一个角度来理解全序广播——用来写日志（比如复制日志、事务日志或者写前日志）：**投递消息就像追加日志**。由于所有节点都会按照同样的顺序发送消息，则所有节点在读取日志的时候也会得到同样的消息序列。

#### 3.3.1 使用全序广播实现线性一致性存储
全序广播是**异步的**：系统保证以同样的**顺序**交付消息，但并不保证消息的交付**时刻**（即，有的消息接收者间可能存在着滞后）。与之相对，线性一致性是一种**新鲜度保证**：读取一定能看到最新成功的写。

1. 向服务中追加一个带有某用户名的消息条目，表明你想使用该用户名。
2. （由于全序广播是异步的）不断读取日志，直到能够读到刚才你追加的消息条目。
3. 检查所有想要使用该用户名的消息，这时你可能会得到多条消息，如果你当初写下的消息在第一条，则你是成功的。此时，你可以“确认”（持久化，比如追加日志，比如写入数据库）占有该用户名的信息，然后给客户端返回成功。如果第一条消息不是你的，则终止请求。

#### 3.3.2 使用线性一致存储实现全序广播
对于每一个发给全序广播系统的消息，使用整数寄存器 increment-and-get 操作关联一个序列号；然后将消息发送给所有节点（重试任何丢失的消息）。每个节点接收到消息后利用序列号顺序对外交付消息。这种机制很像 TCP，但并不是描述通信双方，而是一个分布式系统。

## 4 分布式事务和共识协议
在很多场景下让多个节点达成共识是非常重要的。比如：
- **Leader 选举** 在使用单主模型的数据库中，所有节点需要对谁是主节点达成一致。当网络问题导致有些节点不能正常通信时，领导权就会出现争议。在这种情形下，共识对于避免错误的故障转移非常重要。引入如果出现两个领导者可以同时接受写入（**脑裂**），所有副本上的数据就会产生分叉，从而变得不一致甚而数据丢失。
- **原子提交** 在一个横跨多节点或具有多分区的数据库中，可能会出现某个事务在一些节点执行成功，但在另外一些节点却运行失败。如果我们想保持事务的原子性（ACID 中的 A，参见[原子性](https://ddia.qtmuniao.com/#/ch07?id=%e5%8e%9f%e5%ad%90%e6%80%a7%ef%bc%88atomicity%ef%bc%89)），我们就必须让所有节点就事务的结果达成一致：要么全部回滚（只要有故障），要么提交（没有任何故障）。这个共识的特例也被称为**原子提交**（atomic commit）。
### 4.1 原子提交和两阶段提交
原子性能够避免失败的事务通过半完成（half-finished）或者半更新（half-updated）的结果来破坏数据库系统。

二级索引是独立于**原始数据**的一种数据结构，因此如果你更新了原始数据，对应的二级索引也需要进行同步更新。原子性能够保证二级索引和原始数据时刻保持一致。

#### 4.1.1 从单机到分布式的原子提交
简单地在提交事务时给每个节点发送提交请求让其提交事务，是不能够满足事务基本要求的。这是因为，可能有的节点成功提交了，有的节点却提交失败了，从而违反了原子性保证：
- 有些节点在提交时检测到完整性约束被破坏了，因此中止事务；但另外一些节点却能够成功提交。
- 有些提交请求由于网络过慢而超时丢弃，另外一些提交请求却成功抵达。
- 有一些节点在写入提交记录前宕机重启，导致事务回滚；另外一些节点却成功提交。

**事务提交后是不可撤销的**——在事务提交后，你不能再改变主意说，我要重新中止这个事务。这是因为，一旦事务提交了，就会对其他事务可见，从而可能让其他事务依赖于该事务的结果做出一些新的决策；这个原则构成了**读已提交**（read commited）隔离级别的基础（参见[读已提交](https://ddia.qtmuniao.com/#/ch07?id=%e8%af%bb%e5%b7%b2%e6%8f%90%e4%ba%a4)）。如果事务允许在提交后中止，其他已经读取了该事务结果的事务也会失效，从而引起事务的级联中止。

当然，事务所造成的**结果**在事实上是可以被撤销的，比如，通过**补偿事务**（_compensating transaction_）。但，从数据库的视角

#### 4.1.2 *两阶段提交*
**两阶段提交**（2PC，two-phase commit）是一种在多个节点上实现原子事务的算法——即，保证所有节点要么都提交，要么都中止。   相比单机事务的一次提交请求，2PC 中的提交、中止过程被拆分成了两个阶段

**2PC** 引入了一个单机事务中没有的角色：**协调者**（coordinator，有时也被称为事务管理器，transaction manager）。

**2PC 事务**通常也由应用层对多个节点上的数据读写开始。和协调者相对，我们将这些数据节点称为事务的**参与者**（participants）。当应用层准备好提交后，协调者开始阶段一：向每个参与者发送 **prepare** 请求，询问他们是否能够提交。然后，协调者会根据参与者的返回而进行下一步动作：
1. 如果**所有参与者**都回复“可以”（yes），表示能够提交，则协调者就会进入第二阶段发出**提交**（ **commit** ）请求，此时，提交事实上才开始执行。
2. 如果有任何参与者回复“不行”（no），或者请求超时了，协调者就会进入第二阶段并发送一个 **中止**（abort）请求，中止事务。
#### 4.1.3 基于承诺的系统

1. 当应用想开启一个分布式事务时，它会首先向协调者要一个**事务 ID**。该事务 ID 是全局唯一的。
2. 应用会使用前述事务 ID 向所有的参与者发起一个单机事务，所有节点会各自完成读写请求，在此过程中，如果有任何出错（比如节点宕机或者请求超时），协调者或者任意参与者都可以中止事务。
3. 当应用层准备好提交事务时，协调者会向所有参与者发送**准备提交**（prepare）请求，并在请求中打上事务 ID 标记。如果有请求失败或者超时，则协调者会对所有参与者发送带有该事务 ID 的中止请求。
4. 当参与者收到**准备提交**请求时，它必须确认该事务能够在任何情况下都能被提交，才能回复“**可以**”。这包括，将所有写入刷到磁盘（一旦承诺了，就不能反悔，即使之后遇到宕机、断电或者磁盘空间不足）、检查是否有冲突或者违反约束的情况。换句话说，如果回复“可以”，意味着参与者**让渡了中止事务的权利（给协调者）**，但此时并没有真正地提交。
5. 当协调者收到所有参与者准备提交的回复后，会决定提交还是中止该事务（只有在所有参与者都回复“可以”时，才会提交）。协调者需要将该决策写入事务日志，并下刷到磁盘，以保证即使宕机重启，该决策也不会丢失。这被称为**提交点**（commit point）。
6. 协调者将决策刷入了磁盘后，就会将决策（提交或者中止）请求发给所有参与方。如果某个请求失败或者超时，则协调者会对其进行无限重试，直到成功。不允许走回头路：如果协调者决定了提交，则不管要进行多少次的重试，也必须要保证该决策的执行。如果参与者在此时宕机了，则当重启时也必须进行提交——因为它**承诺过要提交**，因此在重启后不能拒绝提交。

该协议有两个重要的“不可回退点”：
1. 当某个参与者回复“可以”时，就做出了（将来无论发生什么）肯定可以提交的承诺。（当然，协调者可以中止事务）
2. 当协调者决定提交时，该决定一旦做出（写入磁盘），就是不可撤回的。


#### 4.1.4 协调者故障
如果协调者在准备提交请求发送前故障，则参与者可以放心的中止事务。然而，一旦参与者收到准备提交请求，并且回复“可以”，则根据 2PC 设定，它**不能单方面的中止事务**——而必须等待协调者的提交或者中止请求。如果此时协调者宕机或者网络故障，则参与者只能**死等**。参与者事务的这种状态称为**存疑**（in doubt）或者**未定**（uncertain）。

在 2PC 中，唯一使算法能够完成的方法就是等待协调者恢复。这也是为什么，协调者在给参与者发送提交或者中止消息时，需要先将该决策写入事务日志中：当协调者恢复时，他就能从事务日志中读取该决策，以让所有处于未决状态的参与者状态确定下来。如果协调者恢复了，发现并没有写入任何决策到事务日志中，则中止该事务。因此，2PC 的**提交点**（commit point）最终可以归结到协调者上的单机原子提交。



#### 4.1.5 三阶段提交
由于 2PC 在等待协调者宕机恢复时系统可能会卡住，因此两阶段提交又称为**阻塞式原子提交协议**（blocking atomic commit protocol）。
作为 2PC 的替代，人们又提出了三阶段提交（three-phase commit）。然而，3PC 对系统有一定假设：网络具有有界延迟，请求延迟也是有界的（bounded，参见[超时和无界延迟](https://ddia.qtmuniao.com/#/ch08?id=%e8%b6%85%e6%97%b6%e5%92%8c%e6%97%a0%e7%95%8c%e5%bb%b6%e8%bf%9f%ef%bc%88unbounded-delays%ef%bc%89)）。在具有无界网络延迟进程停顿的实际系统中，3PC 无法保证原子性。
在具有无界延迟的网络中，超时机制就不是一个可靠的故障检测方法，即使没有任何节点故障，一个请求仍会由于网络问题而超时。出于这个原因，即使 2PC 可能会因为协调者宕机卡住，但人们仍然在使用它，而没有转向 3PC。

### 4.2 实践中的分布式事务
分布式事务，尤其是使用两阶段提交实现的分布式事务，毁誉参半。一方面，他们可以提供其他方式难以实现的**安全保证**；另一方面，由于运维复杂、降低性能、承诺过多，他们广受诟病。为了避免分布式事务带来的运维复杂度，很多云服务选择不支持分布式事务。

两种完全不同的分布式事务经常被混淆：

- **数据库内部分布式事务** 在一些分布式数据中（标配支持多分区和多副本的数据库），支持跨节点的**内部分布式事务**。如，VoltDB 和 MySQL 集群的 NDB 存储引擎就有这样的内部事务支持。在这种情况下，所有事务参与节点都运行着同样的二进制代码。
- **异构的分布式事务** 在异构的分布式事务中，所有参与者使用了两种以上的技术栈：如，来自不同厂家的两种数据库实例，甚至可能包含非数据库系统，如消息队列。即使每个子系统内部实现完全不同，构建于其上的分布式事务也能够保证原子提交。

数据库内部的事务不需要考虑和其他系统的相容性，因此在实现时可以使用任何协议、可以针对特定技术栈进行任何优化。因此，数据库内部的分布式事务通常能够很好地工作。相反，横跨多个异构系统的事务实现则充满了挑战。

#### 4.2.1 恰好一次的消息处理
异构的分布式事务系统可以将多种异构的系统，以强大的方式进行整合。例如，**当且仅当**数据库中处理消息的事务成功提交时，消息队列才会将该消息标记为**已处理**。可以将消息确认和数据库写入打包在单个事务里进行原子提交，来实现上述行为。

#### 4.2.2 XA 事务
XA 不是一个网络协议——它定义了一组和事务协调者交互的 C 语言 API 接口。
使用事务的**应用层**会以网络驱动（network driver）或者客户端库（client library）来使用 XA 的 API 与参与者服务（数据库或者消息队列）进行交互。如果驱动程序支持 XA 协议，则意味着应用侧可以调用 XA 的 API 来确定一个操作是否是分布式事务的一部分（即通过 XA 定义的接口来确定事务所涵盖**操作的边界**）；如果是，则会发送必要的消息给参与者。XA 驱动也提供了一些回调，协调者可以使用这些回调要求参与者进行准备、提交或者中止。

事务的**协调者**实现了 XA API。XA 的标椎并没规定协调者该如何实现，并且在实践中协调者通常以**库的形式**被加载进应用程序中（作为应用程序的一部分，而非额外单独的一个服务）。它会追踪事务中的所有参与者，在要求参与者准备提交（prepare）后收集其回复，使用本地磁盘上的日志来跟踪每个事务的**提交/中止**决策。

#### 4.2.3 阻塞时持有锁
问题的关键点在于存在**锁**（locking）。数据库中的事务通常会使用行级别的互斥锁来保护对某一行的修改，以防止脏写。更进一步，如果想获得可串行化隔离级别，数据库在使用两阶段锁进行实现时，会对事务所有读过的行加共享锁（参见[两阶段锁](https://ddia.qtmuniao.com/#/ch07?id=%e4%b8%a4%e9%98%b6%e6%ae%b5%e9%94%81)）。
数据库在提交或者中止事务前**不能够释放获取的这些锁**。因此，在使用两阶段提交时，一个事务必须在其处于未定状态期间一直持有锁。如果协调者在宕机后花了 20 分钟才重新启动起来，则对应参与者的锁就要持有 20 分钟。如果参与者日志由于某种原因丢掉了，这些锁会被永远的持有——除非系统管理员会手动释放它们。

#### 4.2.4 从协调者故障中恢复
在实践中，常会产生一些孤立的（orphaned）未定事务——即，由于某种原因，**事务的协调者**（比如由于软件 bug 事务日志丢失或者损坏）**无从判断事务的最终结果是提交还是回滚**。由是，这些事务不能够被自动的处理，从而永久的卡在那里，持有锁并且阻塞其他事务。
唯一的出路是让管理员手动的来提交或者中止事务。
很多 XA 事务的实现会留有紧急后门，称为**启发式决策**（_heuristic decisions_）：允许一个参与者不用等待协调者的决策，而**单方面**决定中止还是提交一个未定事务。需要说明的是，这里的启发式仅仅是**可能打破原子性**（probably breaking atomicity）的一种委婉说法。因为这么做可能会违反两阶段提交所提供的保证。因此这种启发式决策仅是为了救急，而不能进行日常使用。****

#### 4.2.5 分布式事务的限制
分布式事务有**放大故障**的嫌疑，这与我们构建容错系统的目标背道而驰（这就是 tradeoff，为上层提供的更多的一致性保证，就会牺牲性能，降低可用性）。

### 4.3 容错的共识算法
共识协议通常被描述为：一个或者多个节点可能会各自**提议**（propose）一些值，共识协议需要在这些值中间做出唯一的**决策**（decide）。
- **全局一致性**（_Uniform agreement_） 没有任何两个节点最终做出不同决策。
- **正直性**（_Integrity_） 没有任何节点会做出两次决策（不会反复横跳）
- **有效性**（_Validity_） 如果一个节点做出了决策，该决策所对应的值一定来自系统内某个节点的提议
- **可终止性**（_Termination_） 任何没有宕机的节点，最终都会给出对某个值的决策
全局一致和正直性定义了共识协议的核心概念：**所有节点都要决策出同样的结果，并且一旦做出决策，就不能反悔**。加入有效性更多的是为了排除一些无效（trivial）结果


**可终止性是对容错的一种形式化描述**（从结果来描述）。它本质上是在说，一个共识算法不能让系统陷入一种卡在那、啥也不干，直到永远的状态。换句话说，系统必须能够正常运作，即使有些节点宕机，其他节点也必须能够继续做出决策。（可结束性是存活性，liveness，而其他三个性质是安全性，safety，

可终止性受限于少于半数节点宕机或不可达的假设。然而，大多数共识算法的实现在大多数节点都宕机或者网络出现大范围故障时仍然能保持安全性——一致性，正直性和有效性。也即，大范围的节点下线可能会让系统**不能继续处理请求**，但**不会因此破坏共识协议**，让其做出不合法决策。


#### 4.3.1 全序广播中的共识算法

全序广播等价于多轮次的共识协议（每个轮次，会使用共识协议对全序广播中的一条消息的全局顺序做出决策）：

由于共识协议的全局一致性，所有节点会以同样的顺序投递同样的消息。
由于正直性，具有同样 id 的消息不会重复。
由于有效性，消息不会是损坏的，也不会是凭空捏造的。
由于可终止性，消息不会丢失。


#### 4.3.2 单主复制和共识协议
核心点在于**主节点（领导者）是怎样选出的**。如果主节点由运维团队的管理员手动配置，你本质上就获得了一个“共识算法”的独裁变种：只有一个节点允许接受写入（决定复制日志中所有日志的顺序），并且一旦该主节点宕机，系统便会陷入不可用的状态，直到运维人员手动的配置另外一个节点为主节点。这样的系统在实践中也可以正常运作，但是并不满足共识算法中的可终止性，因为它在停顿后要求运维人员的干预，才能继续运转。

有些数据库在遇到主节点故障时，会自动地重新进行主选举，将一个从节点提升为新的主节点（参见[宕机处理](https://ddia.qtmuniao.com/#/ch05?id=%e5%ae%95%e6%9c%ba%e5%a4%84%e7%90%86)）。这就让我们进一步逼近了可容错的全序广播，并且解决了共识问题。

#### 4.3.3 纪元编号和法定人数
协议会定义一个**纪元编号**，且保证在每一个纪元（epoch）内，主节点是唯一的。

每次当前的主节点被认为下线时（可能是宕机，也可能只是网络不通），所有认为该主下线的节点就会发起选举，以选出新的主节点。每次选举会使用一个更高的纪元编号，因此所有的纪元编号是全序且单调递增的。如果不同纪元中有两个节点都认为自己是主（比如之前的主节点并没有宕机），则具有较高纪元编号的主节点胜出。

在一个主节点被授权做任何事之前，它必须要确认不会有更权威的主节点（具有更高的纪元编号）会做出不同决策

因此，主节点在决策前需要首先从所有节点获得法定票数（参见[Quorum 读写](https://ddia.qtmuniao.com/#/ch05?id=quorum-%e8%af%bb%e5%86%99)）。对于每个决策，主节点都必须将其作为提案发给其他所有节点，并且等待法定节点的同意。法定节点通常来说，会包含多数派节点，但也不绝对（[Flexible Paxos](https://arxiv.org/abs/1608.06696)介绍了一种不需要多数节点的放宽的 Paxos 算法）。如果法定节点的回复中没有任何更高纪元的，则当前主节点可以放心的认为没有发生新纪元的主选举，并可以据此认为他仍然“握有领导权”。从而，可以安全的对提案进行决策。

投票过程非常像两阶段提交提交算法。最大的区别在于：

1. 2PC 中的协调者不是被选出来的。
2. 2PC 要求每一个参与者都回复“可以”，而可容错的共识算法只要求多数节点的投票。




#### 4.3.4 共识算法的局限性

**同步复制损失性能**。每次进行决策（更改数据）前都要让多数节点进行投票，意味着这是一个同步复制系统。在[同步复制和异步复制](https://ddia.qtmuniao.com/#/ch05?id=%e5%90%8c%e6%ad%a5%e5%a4%8d%e5%88%b6%e5%92%8c%e5%bc%82%e6%ad%a5%e5%a4%8d%e5%88%b6)一节中我们讲过，很多数据库都会配置为异步复制。在这种配置下，有些已经提交的数据在进行恢复时可能会丢失，但很多人仍然选择这种模式——承担这种风险，以换取更好的性能。

**多数派会增加系统冗余**。共识系统总是要求有**严格多数节点**存活才能正常运行。这意味着，如果你要容忍单节点故障就至少需要三个节点（三节点中的两个节点可以组成多数派），如果要容忍两个节点故障就至少需要五个节点（五个节点中的三个节点组成多数派）。如果网络故障切断了其中一些节点和其他节点的联系，则只有连通的多数派节点可以正常运行，其他节点都会被阻塞。

**动态成员变更复杂**。很多共识算法会假定有固定的数目节点参与投票，这意味着你不能往集群中增删节点。共识算法的**动态成员变更**（dynamic membership）扩展允许集群的节点集随时间推移而发生变动，但相对于静态成员算法，这种扩展版本非常难以理解。

**复杂网络环境性能很差**。共识系统通常通过超时机制来对故障节点进行检测。在延迟高度变化的网络中，尤其是多地部署的分布式系统中，某些存活节点由于网络的瞬时抖动常被误认为发生了故障。尽管这些问题并不会破坏安全性，但频繁的领导者选举会导致极差的性能表现——系统可能会大部分时间都在选主而不是正常干活上。

**共识算法对网络故障非常敏感**。

### 4.4 成员关系和协调服务

Zookeeper 和 etcd 设计目标为**存储小尺度的数据**，比如能装进内存里的

这些系统使用可容错的全序广播算法，将小尺寸的数据被复制到所有节点上。如前所述，我们做数据库复制的时候真正需要的东西其实是全序广播：如果每条消息代表针对数据库的一个修改，以相同的顺序对所有副本应用相同的改动，能够将数据库保持在一致的状态。

Zookeeper 是模仿 Google 的 Chunk 锁服务实现的，不仅实现了全序广播算法（进而实现了共识），也实现了其他一些对分布式系统非常有用的功能集：

- **线性化的原子操作（lock）** 使用原子的 CAS 操作，可以实现锁：如果多个节点并发执行同一个操作，只有一个会成功。共识协议能够保证，即使随时可能出现节点宕机或者网络故障，操作仍然是原子和线性化的。一个分布式锁通常实现为具有**过期时间的“租约”**（lease），这样即使客户端宕机，锁也能够被最终释放。
- **操作的全序保证（zxid）** 在[领导者和锁](https://ddia.qtmuniao.com/#/ch08?id=%e9%a2%86%e5%af%bc%e8%80%85%e5%92%8c%e9%94%81)一节中我们讨论过，当某个资源被锁或者租约保护时，你需要**防护令牌机制**来防止由于进程停顿而造成的加锁冲突。防护令牌一个在每次获取锁都会单调自增的数值。Zookeeper 通过给每个操作赋予一个全局自增的事务 id（zxid）和一个版本号（cversion）来提供该功能。
- **故障检测（ephemeral node）** 客户端和 ZooKeeper 的服务器间维持着一个长会话，客户端和服务端通过周期性的心跳来检测对端是否仍然存活。即使该连接短暂断掉，或者 ZooKeeper 节点故障，该会话仍然能够存活。但如果，心跳停顿间隔过长，超过了会话的超时阈值，ZooKeeper 会标记该会话死亡。所有该会话关联的锁在超时都将会被释放（ZooKeeper 将其称为**暂态节点**，ephemeral nodes，这类节点可以将生命周期与会话进行绑定）。
- **变动通知（watch）** 客户端不仅可以读取其他节点创建的锁或者值，也可以直接对这些对象的变化进行**守望**（watch）。通过守望机制，客户端可以立即发现是否有其他客户端加入集群（通过这些客户端写入 ZooKeeper 的值）、其他客户端是否故障（通过这些客户端注册到 ZooKeeper 中的暂态节点的消失）。通过订阅这些通知，客户端可以避免频繁地去 ZooKeeper 拉取信息，比对以确定是否发生了某些变化。


#### 4.4.1 为节点分配任务

ooKeeper 通常运行在固定节点的集群上（通常是三个或者五个），并且只须在这几个节点间达成共识，然后就可以支持非常多的客户端访问。这样，ZooKeeper 提供了一种可以将**部分功能**（共识算法、外包定序、故障检测）“**外包**”（outsouring）给外部服务的方法。

#### 4.4.2 服务发现
即根据服务名称找到其对应的 IP 地址以进行连接。在数据中心的环境中，虚拟机的来来去去非常普遍，因此很难事先知道某个服务的 IP 地址。因此，你可以对服务进行配置，让其在启动的时候在**某个服务**（通常是名字服务器，nameserver）注册自己的地址和端口，其他人就能使用名字来找到该服务的最终地址。

#### 4.4.3 成员服务
成员服务可以确定当前**集群中哪些节点当前是存活的**。